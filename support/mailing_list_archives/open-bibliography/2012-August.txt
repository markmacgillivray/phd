From owen at ostephens.com  Fri Aug  3 10:43:21 2012
From: owen at ostephens.com (Owen Stephens)
Date: Fri, 3 Aug 2012 10:43:21 +0100
Subject: [open-bibliography] Open data and Research Libraries UK
Message-ID: <0C18F436-380A-4A8F-86DF-A0685C8A4370@ostephens.com>

Hello all

I've been commissioned by Research Libraries UK (RLUK) to look at the possibility of making RLUK data openly available, and the related issues and challenges. As part of this work it is important for us to understand who the audience for such open data might be, how they might use the data, and what licences, formats and mechanisms will best support this use. I hope you are able to help by completing the survey linked below.

To give a bit more detail on the data we are talking about. Research Libraries UK, through JISC and MIMAS, makes available a large database of bibliographic data. RLUK estimates that approximately 16 million bibliographic records in its database are free from restrictions in terms of redistribution and open licensing. 

RLUK is committed to the principle of open bibliographic data, and is a signatory to the JISC Discovery Open Metadata Principles (http://discovery.ac.uk/businesscase/principles/). RLUK would therefore like to determine the most effective way of publishing the available records as open metadata, with an emphasis on enabling reuse. 

The survey should only take about 10 minutes to complete and is available at: https://www.surveymonkey.com/s/5RH8KH8

Thanks and best wishes

Owen

Owen Stephens
Owen Stephens Consulting
Web: http://www.ostephens.com
Email: owen at ostephens.com
Telephone: 0121 288 6936

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120803/9aed880d/attachment.htm>

From lists at kcoyle.net  Fri Aug  3 17:07:15 2012
From: lists at kcoyle.net (Karen Coyle)
Date: Fri, 03 Aug 2012 09:07:15 -0700
Subject: [open-bibliography] Fwd: [CODE4LIB] Open data and Research
	Libraries UK
In-Reply-To: <C3989CEF-E032-49C2-A5AC-A0148B50A6DD@ostephens.com>
References: <C3989CEF-E032-49C2-A5AC-A0148B50A6DD@ostephens.com>
Message-ID: <501BF733.6030208@kcoyle.net>

I hope some of you will find time to answer this survey. - kc


-------- Original Message --------
Subject: 	[CODE4LIB] Open data and Research Libraries UK
Date: 	Fri, 3 Aug 2012 10:43:53 +0100
From: 	Owen Stephens <owen at OSTEPHENS.COM>
Reply-To: 	Code for Libraries <CODE4LIB at LISTSERV.ND.EDU>
To: 	CODE4LIB at LISTSERV.ND.EDU



Hello all

I've been commissioned by Research Libraries UK (RLUK) to look at the possibility of making RLUK data openly available, and the related issues and challenges. As part of this work it is important for us to understand who the audience for such open data might be, how they might use the data, and what licences, formats and mechanisms will best support this use. I hope you are able to help by completing the survey linked below.

To give a bit more detail on the data we are talking about. Research Libraries UK, through JISC and MIMAS, makes available a large database of bibliographic data. RLUK estimates that approximately 16 million bibliographic records in its database are free from restrictions in terms of redistribution and open licensing.

RLUK is committed to the principle of open bibliographic data, and is a signatory to the JISC Discovery Open Metadata Principles (http://discovery.ac.uk/businesscase/principles/). RLUK would therefore like to determine the most effective way of publishing the available records as open metadata, with an emphasis on enabling reuse.

The survey should only take about 10 minutes to complete and is available at: https://www.surveymonkey.com/s/5RH8KH8

Thanks and best wishes

Owen

Owen Stephens
Owen Stephens Consulting
Web: http://www.ostephens.com
Email: owen at ostephens.com
Telephone: 0121 288 6936




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120803/c267f0f5/attachment.htm>

From naomi.lillie at okfn.org  Mon Aug  6 19:13:40 2012
From: naomi.lillie at okfn.org (Naomi Lillie)
Date: Mon, 6 Aug 2012 19:13:40 +0100
Subject: [open-bibliography] Next virtual meeting: 2012-08-07, 15:00 GMT
Message-ID: <CACGRf4LNxDD20XtyX0NvyFXVNVbtqLLKOLJB2AfJxrbtLefr7Q@mail.gmail.com>

Dear all,

In Adrian's absence I'll be hosting the Open Bibliography Working Group
call, tomorrow Tuesday 7th August at 16.00 BST (GMT+1).

Please log into the etherpad <
http://okfnpad.org/24th-open-bibliography-meeting> a couple of minutes
before and I'll call you on Skype (please add your Skype details to the
pad); please do also add your topics for discussion.

Regards,
Naomi


-- 
Naomi Lillie
Foundation Administrator and Community Coordinator (Open Bibliography)
Open Knowledge Foundation
http://okfn.org/
Skype: n.lillie
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120806/ff1d19e4/attachment.htm>

From naomi.lillie at okfn.org  Tue Aug  7 09:57:46 2012
From: naomi.lillie at okfn.org (Naomi Lillie)
Date: Tue, 7 Aug 2012 09:57:46 +0100
Subject: [open-bibliography] Fwd: [Open-access] Get your OKFest tickets
 today - early bird prices end TOMORROW!
In-Reply-To: <CAMQ+bMYqfXnLVqauKFSM92_ycfDY78NKsFqh+o9yAmORcGrxSQ@mail.gmail.com>
References: <CAMQ+bMYqfXnLVqauKFSM92_ycfDY78NKsFqh+o9yAmORcGrxSQ@mail.gmail.com>
Message-ID: <CACGRf4+9oWjFJe-vgAKvfLMgm0c+yOVz0P5a0MdJySJR3ZSvhQ@mail.gmail.com>

*Please excuse the cross-posting*

Hi all, if you haven't bought your ticket for OKFest yet please see below
for a reminder! Naomi


---------- Forwarded message ----------
From: Laura Newman <laura.newman at okfn.org>
Date: 7 August 2012 09:49
Subject: [Open-access] Get your OKFest tickets today - early bird prices
end TOMORROW!
To: open-science <open-science at lists.okfn.org>, open-access at lists.okfn.org


Hi all,

A reminder that OKFest early bird ticket sales close TOMORROW. To reserve
your ticket at the discounted rate, visit:
http://okfestival.org/early-bird-okfest-tickets/ now!

For details of all that the Research and Education stream has planned, see
this blog post:
http://science.okfn.org/2012/08/07/get-your-ticket-for-okfest-today-early-bird-closes-august-8th/

We have a great two days lined up on Wednesday 19th and Thursday 20th
September. Sessions will include a keynote by Mat Todd on Open Source Drug
Discovery, an interactive session with Carl-Christian Bohr to discuss the
European Commission's open science policy, and panel discussions on
communicating science (including representatives from academia, science
publishing and wikimedia), making open access work in practice, and opening
up raw experimental data (led by Mark Hahnel from Figshare). There will
also be a host of workshops, hands-on sessions and drafting sprints, so do
come and join us!

On Tuesday 18th September, we will be holding a satellite hackday. Details
here:
https://docs.google.com/spreadsheet/viewform?formkey=dENfTWM3S0JtVUVOazNNZ2R3Um5WQ1E6MQ
.

I hope to see you there!

Kind regards,
Laura


-- 
Laura Newman
Community Coordinator
Open Knowledge Foundation
http://okfn.org/
Skype: lauranewmanonskype
Twitter: @Newmanlk


_______________________________________________
open-access mailing list
open-access at lists.okfn.org
http://lists.okfn.org/mailman/listinfo/open-access




-- 
Naomi Lillie
Foundation Administrator and Community Coordinator
Open Knowledge Foundation
http://okfn.org/
Skype: n.lillie
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120807/365ce6c6/attachment.htm>

From kcoyle at kcoyle.net  Wed Aug 15 15:43:32 2012
From: kcoyle at kcoyle.net (Karen Coyle)
Date: Wed, 15 Aug 2012 07:43:32 -0700
Subject: [open-bibliography] Fwd: FW: OCLC provides downloadable linked data
 file for the 1 million most widely held works in WorldCat
In-Reply-To: <013f01cd7a44$68e0a510$3aa1ef30$@com>
References: <013f01cd7a44$68e0a510$3aa1ef30$@com>
Message-ID: <502BB594.30108@kcoyle.net>



*From:*Murphy,Bob [mailto:murphyb at oclc.org]
*Sent:* Tuesday, August 14, 2012 12:34 PM
*To:* Murphy,Bob
*Subject:* OCLC provides downloadable linked data file for the 1 million
most widely held works in WorldCat

*_FOR IMMEDIATE RELEASE_*

**

*FOR MORE INFORMATION:*

Bob Murphy +1-614-761-5136

murphyb at oclc.org <mailto:murphyb at oclc.org>

**

*
**OCLC provides downloadable linked data file for the
1 million most widely held works in WorldCat*

//

DUBLIN, Ohio, August 14, 2012?OCLC has published bibliographic linked
data for the most widely held works in WorldCat. This downloadable
file?representing nearly 1.2 million resources?contains approximately 80
million linked data ?triples,? the term for the most granular
relationship possible between discrete pieces of information.

?This is an important step for libraries and linked data,? said Richard
Wallis, OCLC Technology Evangelist. ?Organizations wishing to develop
linked data services can experiment with this data set before going into
full development. They?ll also be able to stress-test new services using
a very large and important set of up-to-date, linked library data. We
are really interested to see what people will do with this data.?

The linked data is provided as RDF serialization, and uses the
Schema.org ontology as well as library extensions to Schema.org that
OCLC has been working on with members and partners over the last year.
It is being made available, under an ODC-BY data license
<http://www.oclc.org/data/attribution.html>, in a single, 1-gigabyte,
compressed (GZip) file, which can be downloaded from here
<http://purl.oclc.org/dataset/WorldCat/datadumps/WorldCatMostHighlyHeld-2012-05-15.nt.gz>. 



While WorldCat contains bibliographic records for more than 275 million
items, the choice was made to select the most widely held materials for
this release in order to help keep the file at a manageable size. Jeff
Young, the OCLC Research software architect who did much of the modeling
necessary to generate the linked data file, explains, ?To make the cut,
a resource had to be held by at least 250 libraries. This seemed to us
to be a good balance between providing widely useful data while making
it reasonably manageable for most uses.?

?OCLC expects that the file will be useful as a source of raw data.
Information about works, authors and publishers can be dissected and
recombined in this format much more easily,? Mr. Young said. ?This
provides a great tool for researchers in library science, as well as
those who may want to do cultural, historical, sociological or other
research based on the rich data libraries have been contributing to
WorldCat for decades.?

Mike Teets, OCLC Vice President for Innovation, added, ?This release
will make it easier for the wider linked data community?commercial
providers, retail organizations, researchers and scholars?to include
library information in their workflows. It will also make it easier for
libraries to do the same in reverse, connecting their materials back to
the Web through services that people use every day.?

In June 2012, OCLC added Schema.org tags to WorldCat.org records,
improving the way in which library information is represented to search
engines. OCLC has also developed linked data resources for the Dewey
Decimal Classification System <http://dewey.info/>, FAST
<http://www.oclc.org/research/activities/fast/default.htm> (Faceted
Application of Subject Terminology) and the VIAF <http://viaf.org/>
(Virtual International Authority File) service. The release of these 1.2
million records as linked data is the next step in OCLC?s linked data
strategy.

?We are focusing our efforts on getting WorldCat data into accessible
forms for local experimentation and development,? explained Mr. Teets,
?with the objective that this will promote libraries as a trusted hub
for linked data.?

?This really is an effort that requires input from many sources,?
continued Mr. Teets. ?Designing and generating linked data in this way
requires many vocabulary and modeling choices, and we want to get as
much input and commentary from the library community as possible.?

To take part in the discussion about library linked data, sign up to
participate in the linked data discussion forum on the OCLC Developer
Network <http://www.oclc.org/developer/groups/linked-data> or send your
thoughts via e-mail to data at oclc.org <mailto:data at oclc.org>.

**

*About OCLC*

Founded in 1967, OCLC is a nonprofit, membership, computer library
service and research organization dedicated to the public purposes of
furthering access to the world?s information and reducing library costs.
More than 72,000 libraries in 170 countries have used OCLC services to
locate, acquire, catalog, lend, preserve and manage library materials.
Researchers, students, faculty, scholars, professional librarians and
other information seekers use OCLC services to obtain bibliographic,
abstract and full-text information when and where they need it. OCLC and
its member libraries cooperatively produce and maintain WorldCat, the
world?s largest online database for discovery of library resources.
Search WorldCat.org <http://www.worldcat.org/> on the Web. For more
information, visit the OCLC website <http://www.oclc.org/us/en/default.htm>.

Dewey, Dewey Decimal Classification, FAST, OCLC, VIAF, WorldCat and
WorldCat.org are trademarks/service

marks of OCLC Online Computer Library Center, Inc.

Third-party product, service and business names are trademarks/service
marks of their respective owners.

-0-





From announce at dublincore.net  Wed Aug 15 18:53:34 2012
From: announce at dublincore.net (DCMI Announce)
Date: Wed, 15 Aug 2012 10:53:34 -0700
Subject: [open-bibliography] NISO/DCMI Webinar: Metadata for Managing
	Scientific Research Data
Message-ID: <CAP884Exr8p-1sLtq+05Vj7quQzhbnVc8gqbxKtfwD5skf1mv2Q@mail.gmail.com>

******** PLEASE EXCUSE THE CROSS-POSTING ********

WEBINAR: NISO/DCMI Webinar: Metadata for Managing Scientific Research Data

DATE: August 22, 2012
TIME: 1:00 - 2:30 p.m. (Eastern Time)
EVENT WEBPAGE: http://www.niso.org/news/events/2012/dcmi/scientific_data/

ABOUT THE WEBINAR

The past few years have seen increased attention to national and
international policies for data archiving and sharing. Chief motivators
include the proliferation of digital data and a growing interest in
research data and supplemental information as a part of the framework for
scholarly communication. Key objectives include not only preservation of
scientific research data, but making data accessible to verify research
findings and support the reuse and repurposing of data.

Metadata figures prominently in these undertakings, and is critical for the
success of any data repositories or archiving initiative, hence increased
attention to metadata for scientific data -- specifically for metadata
standards development and interoperability, data curation and metadata
generation processes, data identifiers, name authority control (for
scientists), Linked Data, ontology and vocabulary work, and data citation
standards.

This NISO/DCMI webinar will provide a historical perspective and an
overview of current metadata practices for managing scientific data, with
examples drawn from operational repositories and community-driven data
science initiatives. It will discuss challenges and potential solutions for
metadata generation, identifiers, name authority control, Linked Data, and
data citation.

SPEAKERS

Jane Greenberg, professor at the School of Information and Library Science
(SILS) at the University of North Carolina at Chapel Hill and director of
the SILS Metadata Research Center, is well known for research and writing
on topics ranging from automatic metadata creation to metadata best
practices, ontology research, Semantic Web, data repositories, thesauri,
and scientific data curation. She has served as Principal Investigator or
partner on a number of grants from the Institute of Museum and Library
Services, National Science Foundation, and the National Institute of
Health, and actively participates in organizations such as the American
Library Association, American Society for Information Science and
Technology, and the Dublin Core Metadata Initiative. Jane is the recipient
of the 2012 Margaret Mann Citation from the Association for Library
Collections & Technical Services (ALCTS).

Thomas Baker, Chief Information Officer of the Dublin Core Metadata
Initiative, has recently co-chaired the W3C Semantic Web Deployment Working
Group and the W3C Incubator Group on Library Linked Data.

REGISTRATION

Registration is per site (access for one computer) and closes at 12:00 pm
Eastern on August 22, 2012. Discounts are available for NISO and DCMI
members and students.

Can?t make it on the webinar date/time? Register now and gain access to the
recorded archive for one year.

Visit the event webpage to register and for more information:
http://www.niso.org/news/events/2012/dcmi/scientific_data/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120815/40215bbe/attachment.htm>

From johnsonbjeff at gmail.com  Thu Aug 16 21:25:38 2012
From: johnsonbjeff at gmail.com (Bo Johnson)
Date: Thu, 16 Aug 2012 15:25:38 -0500
Subject: [open-bibliography] Hello everyone
Message-ID: <CA+P1A+98Cydm04849KxsB1zBAarqVNUmFbwAcNKrXuyOt-EWuA@mail.gmail.com>

Hello everyone,

My name is Bo Johnson, I currently live in Chicago, Illinois in the US. I
work in information management and am finishing a masters degree in Library
and Information Science. Tech wise, I am not as savvy as I would like to
be, but I have a little knowledge of HTML, CSS and XML, very little
knowledge of Javascript, and almost none with Python, though I am working
through the codeacademy.com courses on Javascript and Python.

I have completed one course in general cataloging (MARC, AACR2, and basic
LCSH and Sears subject cataloging) and one course on metadata for internet
(XML and MARCXML, and very cursory work with EAD, LOM, Dublin Core, METS,
MODS and ONYX). I also used to work as a professional taxonomist, focusing
in faceting.

Most of my experience is in enterprise settings (enterprise content and
document management, specifically) and I would like to contribute in any
way I can.

Thanks everyone!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120816/d5d8c19b/attachment.htm>

From pm286 at cam.ac.uk  Thu Aug 16 21:38:25 2012
From: pm286 at cam.ac.uk (Peter Murray-Rust)
Date: Thu, 16 Aug 2012 21:38:25 +0100
Subject: [open-bibliography] Hello everyone
In-Reply-To: <CA+P1A+98Cydm04849KxsB1zBAarqVNUmFbwAcNKrXuyOt-EWuA@mail.gmail.com>
References: <CA+P1A+98Cydm04849KxsB1zBAarqVNUmFbwAcNKrXuyOt-EWuA@mail.gmail.com>
Message-ID: <CAD2k14OsjuGdK4PjxEOMqwxCz2YeuFDEWnCO0JORnyP9bGuDEg@mail.gmail.com>

On Thu, Aug 16, 2012 at 9:25 PM, Bo Johnson <johnsonbjeff at gmail.com> wrote:

> Hello everyone,
>
> Greetings Bo,


> My name is Bo Johnson, I currently live in Chicago, Illinois in the US. I
> work in information management and am finishing a masters degree in Library
> and Information Science. Tech wise, I am not as savvy as I would like to
> be, but I have a little knowledge of HTML, CSS and XML, very little
> knowledge of Javascript, and almost none with Python, though I am working
> through the codeacademy.com courses on Javascript and Python.
>

There's lots that can be done with simple coding. Much of the work is
things like writing parsers and that's mainly working out the horrors of
legacy formats. A knowledge of regexes, filesystems, servers is always
useful. Javascript is the  main front-end language

>
> I have completed one course in general cataloging (MARC, AACR2, and basic
> LCSH and Sears subject cataloging) and one course on metadata for internet
> (XML and MARCXML, and very cursory work with EAD, LOM, Dublin Core, METS,
> MODS and ONYX). I also used to work as a professional taxonomist, focusing
> in faceting.
>

BibJSON takes the view that simple is beautiful , especially when we need
non-professionals to be involved.

>
> Most of my experience is in enterprise settings (enterprise content and
> document management, specifically) and I would like to contribute in any
> way I can.
>
>
>
-- 
Peter Murray-Rust
Reader in Molecular Informatics
Unilever Centre, Dep. Of Chemistry
University of Cambridge
CB2 1EW, UK
+44-1223-763069
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120816/b56e6885/attachment.htm>

From jcmcoppice12 at gmail.com  Fri Aug 17 13:09:44 2012
From: jcmcoppice12 at gmail.com (Jenny Molloy)
Date: Fri, 17 Aug 2012 13:09:44 +0100
Subject: [open-bibliography] Fwd: [open-science] fw: Python NLTK/data
 mining/machine learning project of public research data, anyone interested?
In-Reply-To: <87has1n7sc.fsf@pobox.com>
References: <mailman.17.1345197601.15227.trizpug@python.org>
	<87has1n7sc.fsf@pobox.com>
Message-ID: <CABPM+woF6+ytXYCtYp07P2feFgwutSpkTgK-59+ckoEeYHhYmw@mail.gmail.com>

Hi All

Apologies for cross-posting but this came out on open-science and I thought
it might be of interest to some of you as well

Jenny

---------- Forwarded message ----------
From: Tom Roche <Tom_Roche at pobox.com>
Date: Fri, Aug 17, 2012 at 11:15 AM
Subject: [open-science] fw: Python NLTK/data mining/machine learning
project of public research data, anyone interested?
To: open-science at lists.okfn.org



Dunno if the following is OT for this group, but thought this thread
from the local PUG might be of interest. (Note I don't know the
author personally; reply to him, not me.)

http://mail.python.org/pipermail/trizpug/2012-August/001919.html
> Nathan Rice nathan.alexander.rice at gmail.com
> Thu Aug 16 20:31:00 CEST 2012

> Hi All,

> Normally, my projects are pretty boring, and I prefer to endure the
> suffering in solitary silence. As luck would have it though, I
> actually have an interesting project on my plate currently, and I
> think it is cool enough that I wanted to give other people the
> opportunity to stick their noses in and provide input or play with
> some code.

> I am currently involved in compiling a database of medical data
> (published clinical or pre-clinical trials) surrounding ethno- and
> alternative- medicinal treatments, for semi-automated meta analysis
> and treatment guidance. In order for this to work, a lot of
> technical challenges have to be overcome:

> My initial tally from PubMed puts the number of articles at over
> 70,000; based on visual inspection, many of these are not actually
> applicable, but there are limited filtering options via the Entrez
> web API. Machine learning techniques would probably be very helpful
> at scoring articles for applicability, and ignoring ones that are
> clearly inapplicable.

> In order to perform meta-analysis and treatment guidance, the
> article needs to be mined for treatment, condition, circumstances of
> treatment and condition, and whether it was successful or not (with
> some p value and sample size). Most of this is not available as
> standard metadata for the studies, and must be mined from the text.

> In addition, not all studies are equal. Methodological errors, lack
> of reproduciblity, and so forth can all render a study meaningless.
> Thus, studies must have a scoring mechanism so you can avoid
> tainting meta-analyses with biased data. This scoring mechanism will
> probably include the impact factor of the journal, the g/h-index of
> the authors, the number of (non self) citations, etc.

> As you can see, each of these is meaty, and all of them need to be
> taken care of to get good results :) If anyone is interested in
> getting some serious natural language processing/data mining/machine
> learning practice, I'd love to involve you. There's no reason I
> should have all the fun!

http://mail.python.org/pipermail/trizpug/2012-August/001920.html
> I'm still in the planning stages for most of the stuff; I have the
> pubmed extraction code pretty well nailed, and I have a solid
> outline for the article disqualification (create a feature vector
> out of topic and abstract bigrams, MeSH subject headings and
> journal, use a SVM discriminator and manually generate a RoC curve
> to determine the cutoff score) but I'm still very up in the air
> regarding NL extraction of things like sample size, significance,
> etc. If you'd like to learn more I would of course be happy to go
> over my thoughts on the matter and we can play around with some
> code.

_______________________________________________
open-science mailing list
open-science at lists.okfn.org
http://lists.okfn.org/mailman/listinfo/open-science
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120817/89d6226b/attachment.htm>

From a.clearn at stmarys-belfast.ac.uk  Fri Aug 17 15:25:23 2012
From: a.clearn at stmarys-belfast.ac.uk (Anthony Clearn)
Date: Fri, 17 Aug 2012 15:25:23 +0100
Subject: [open-bibliography] Hello everyone
In-Reply-To: <CA+P1A+98Cydm04849KxsB1zBAarqVNUmFbwAcNKrXuyOt-EWuA@mail.gmail.com>
References: <CA+P1A+98Cydm04849KxsB1zBAarqVNUmFbwAcNKrXuyOt-EWuA@mail.gmail.com>
Message-ID: <2BC328D2FD9ED342BA3FC9375F4B36B41A517BE07F@vcEX1b.stmarys-belfast.ac.uk>

You might be interested in http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-00-introduction-to-computer-science-and-programming-fall-2008/video-lectures/

and almost none with Python, though I am working through the codeacademy.com<http://codeacademy.com> courses on Javascript and Python.

#####################################################################################
This e-mail message has been scanned for Viruses and Content and cleared 
by MailMarshal
#####################################################################################
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120817/076dc0c9/attachment.htm>

From jonathan.gray at okfn.org  Fri Aug 17 16:58:38 2012
From: jonathan.gray at okfn.org (Jonathan Gray)
Date: Fri, 17 Aug 2012 17:58:38 +0200
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data Piece
	of WorldCat to Play With
In-Reply-To: <CAD47Kz7kiBbweXRQ0-zE5u8roPKTG9fG-3FvRZmZRohcq1DEbg@mail.gmail.com>
References: <CAD47Kz7kiBbweXRQ0-zE5u8roPKTG9fG-3FvRZmZRohcq1DEbg@mail.gmail.com>
Message-ID: <CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>

---------- Forwarded message ----------
From: Richard Wallis <richard.wallis at dataliberate.com>
Date: Fri, Aug 17, 2012 at 5:42 PM
Subject: [LODLAM] Get Yourself a Linked Data Piece of WorldCat to Play With
To: lod-lam at googlegroups.com


In case you missed the press release earlier this week.

You can now download a significant number of RDF triples describing
the most highly held 1.2 million resources in WorldCat.  Licensed
under ODC-BY.

I've posted more details on my blog:
http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-worldcat-to-play-with/

~Richard

--




-- 
Jonathan Gray

Head of Community
The Open Knowledge Foundation
http://www.okfn.org

http://twitter.com/jwyg


From kcoyle at kcoyle.net  Fri Aug 17 17:48:17 2012
From: kcoyle at kcoyle.net (Karen Coyle)
Date: Fri, 17 Aug 2012 09:48:17 -0700
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
 Piece of WorldCat to Play With
In-Reply-To: <CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>
References: <CAD47Kz7kiBbweXRQ0-zE5u8roPKTG9fG-3FvRZmZRohcq1DEbg@mail.gmail.com>
	<CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>
Message-ID: <502E75D1.1040803@kcoyle.net>

I would love it if someone could put this in a triple store for others 
to play with. How difficult is that?

kc

On 8/17/12 8:58 AM, Jonathan Gray wrote:
> ---------- Forwarded message ----------
> From: Richard Wallis <richard.wallis at dataliberate.com>
> Date: Fri, Aug 17, 2012 at 5:42 PM
> Subject: [LODLAM] Get Yourself a Linked Data Piece of WorldCat to Play With
> To: lod-lam at googlegroups.com
>
>
> In case you missed the press release earlier this week.
>
> You can now download a significant number of RDF triples describing
> the most highly held 1.2 million resources in WorldCat.  Licensed
> under ODC-BY.
>
> I've posted more details on my blog:
> http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-worldcat-to-play-with/
>
> ~Richard
>
> --
>
>
>
>

-- 
Karen Coyle
kcoyle at kcoyle.net http://kcoyle.net
ph: 1-510-540-7596
m: 1-510-435-8234
skype: kcoylenet


From lgovro at gmail.com  Fri Aug 17 17:53:33 2012
From: lgovro at gmail.com (Luc Gauvreau)
Date: Fri, 17 Aug 2012 12:53:33 -0400
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
 Piece of WorldCat to Play With
In-Reply-To: <502E75D1.1040803@kcoyle.net>
References: <CAD47Kz7kiBbweXRQ0-zE5u8roPKTG9fG-3FvRZmZRohcq1DEbg@mail.gmail.com>
	<CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>
	<502E75D1.1040803@kcoyle.net>
Message-ID: <CAJcYwHaGXKjVQyCfDjp0FDHjdcRX9hWACkxDBZXgS6D6eMd2bA@mail.gmail.com>

Bonjour,

A very good question!

Multiple projects about linked datas and RDF, but who has the expertise to
use it?

Only experts and "geeks"?

Is it possible for an "amateur" to use these kind of format, files and
codes?

A kind of "Linked data and RDF for dummies" will be very usefull.
Merci,

Luc Gauvreau
(Montr?al, Qu?bec)



2012/8/17 Karen Coyle <kcoyle at kcoyle.net>

> I would love it if someone could put this in a triple store for others to
> play with. How difficult is that?
>
> kc
>
>
> On 8/17/12 8:58 AM, Jonathan Gray wrote:
>
>> ---------- Forwarded message ----------
>> From: Richard Wallis <richard.wallis at dataliberate.**com<richard.wallis at dataliberate.com>
>> >
>> Date: Fri, Aug 17, 2012 at 5:42 PM
>> Subject: [LODLAM] Get Yourself a Linked Data Piece of WorldCat to Play
>> With
>> To: lod-lam at googlegroups.com
>>
>>
>> In case you missed the press release earlier this week.
>>
>> You can now download a significant number of RDF triples describing
>> the most highly held 1.2 million resources in WorldCat.  Licensed
>> under ODC-BY.
>>
>> I've posted more details on my blog:
>> http://dataliberate.com/2012/**08/get-yourself-a-linked-data-**
>> piece-of-worldcat-to-play-**with/<http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-worldcat-to-play-with/>
>>
>> ~Richard
>>
>> --
>>
>>
>>
>>
>>
> --
> Karen Coyle
> kcoyle at kcoyle.net http://kcoyle.net
> ph: 1-510-540-7596
> m: 1-510-435-8234
> skype: kcoylenet
>
>
> ______________________________**_________________
> open-bibliography mailing list
> open-bibliography at lists.okfn.**org <open-bibliography at lists.okfn.org>
> http://lists.okfn.org/mailman/**listinfo/open-bibliography<http://lists.okfn.org/mailman/listinfo/open-bibliography>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120817/e547b7a3/attachment-0001.htm>

From johnson.tom at gmail.com  Fri Aug 17 17:58:35 2012
From: johnson.tom at gmail.com (Tom Johnson)
Date: Fri, 17 Aug 2012 09:58:35 -0700
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
 Piece of WorldCat to Play With
In-Reply-To: <CAJcYwHaGXKjVQyCfDjp0FDHjdcRX9hWACkxDBZXgS6D6eMd2bA@mail.gmail.com>
References: <CAD47Kz7kiBbweXRQ0-zE5u8roPKTG9fG-3FvRZmZRohcq1DEbg@mail.gmail.com>
	<CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>
	<502E75D1.1040803@kcoyle.net>
	<CAJcYwHaGXKjVQyCfDjp0FDHjdcRX9hWACkxDBZXgS6D6eMd2bA@mail.gmail.com>
Message-ID: <CAJeHiNGdwr-iLY4iZf+LaMC=P2g5Q5V+Eon_NF5VbZ0OxTj9AA@mail.gmail.com>

I'm in the process of putting up a triplestore w/ endpoint already. I have
no problem sending out a link.

I'm in an all day meeting today, so it might not happen until the weekend.

On Fri, Aug 17, 2012 at 9:53 AM, Luc Gauvreau <lgovro at gmail.com> wrote:

> Bonjour,
>
> A very good question!
>
> Multiple projects about linked datas and RDF, but who has the expertise to
> use it?
>
> Only experts and "geeks"?
>
> Is it possible for an "amateur" to use these kind of format, files and
> codes?
>
> A kind of "Linked data and RDF for dummies" will be very usefull.
> Merci,
>
> Luc Gauvreau
> (Montr?al, Qu?bec)
>
>
>
> 2012/8/17 Karen Coyle <kcoyle at kcoyle.net>
>
>> I would love it if someone could put this in a triple store for others to
>> play with. How difficult is that?
>>
>> kc
>>
>>
>> On 8/17/12 8:58 AM, Jonathan Gray wrote:
>>
>>> ---------- Forwarded message ----------
>>> From: Richard Wallis <richard.wallis at dataliberate.**com<richard.wallis at dataliberate.com>
>>> >
>>> Date: Fri, Aug 17, 2012 at 5:42 PM
>>> Subject: [LODLAM] Get Yourself a Linked Data Piece of WorldCat to Play
>>> With
>>> To: lod-lam at googlegroups.com
>>>
>>>
>>> In case you missed the press release earlier this week.
>>>
>>> You can now download a significant number of RDF triples describing
>>> the most highly held 1.2 million resources in WorldCat.  Licensed
>>> under ODC-BY.
>>>
>>> I've posted more details on my blog:
>>> http://dataliberate.com/2012/**08/get-yourself-a-linked-data-**
>>> piece-of-worldcat-to-play-**with/<http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-worldcat-to-play-with/>
>>>
>>> ~Richard
>>>
>>> --
>>>
>>>
>>>
>>>
>>>
>> --
>> Karen Coyle
>> kcoyle at kcoyle.net http://kcoyle.net
>> ph: 1-510-540-7596
>> m: 1-510-435-8234
>> skype: kcoylenet
>>
>>
>> ______________________________**_________________
>> open-bibliography mailing list
>> open-bibliography at lists.okfn.**org <open-bibliography at lists.okfn.org>
>> http://lists.okfn.org/mailman/**listinfo/open-bibliography<http://lists.okfn.org/mailman/listinfo/open-bibliography>
>>
>
>


-- 
-Tom Johnson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120817/09917686/attachment.htm>

From thomas.johnson at oregonstate.edu  Fri Aug 17 18:31:52 2012
From: thomas.johnson at oregonstate.edu (Tom Johnson)
Date: Fri, 17 Aug 2012 10:31:52 -0700
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
 Piece of WorldCat to Play With
In-Reply-To: <CAJcYwHaGXKjVQyCfDjp0FDHjdcRX9hWACkxDBZXgS6D6eMd2bA@mail.gmail.com>
References: <CAD47Kz7kiBbweXRQ0-zE5u8roPKTG9fG-3FvRZmZRohcq1DEbg@mail.gmail.com>
	<CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>
	<502E75D1.1040803@kcoyle.net>
	<CAJcYwHaGXKjVQyCfDjp0FDHjdcRX9hWACkxDBZXgS6D6eMd2bA@mail.gmail.com>
Message-ID: <CAJeHiNHJw-9JiX+=CU5YWSMunNJaBWB8htB6FhuBMvvSfwy5Fg@mail.gmail.com>

I'm in the process of putting up a triplestore w/ endpoint already. I have
no problem sending out a link.

I'm in an all day meeting today, so it might not happen until the weekend.

On Fri, Aug 17, 2012 at 9:53 AM, Luc Gauvreau <lgovro at gmail.com> wrote:

> Bonjour,
>
> A very good question!
>
> Multiple projects about linked datas and RDF, but who has the expertise to
> use it?
>
> Only experts and "geeks"?
>
> Is it possible for an "amateur" to use these kind of format, files and
> codes?
>
> A kind of "Linked data and RDF for dummies" will be very usefull.
> Merci,
>
> Luc Gauvreau
> (Montr?al, Qu?bec)
>
>
>
> 2012/8/17 Karen Coyle <kcoyle at kcoyle.net>
>
>> I would love it if someone could put this in a triple store for others to
>> play with. How difficult is that?
>>
>> kc
>>
>>
>> On 8/17/12 8:58 AM, Jonathan Gray wrote:
>>
>>> ---------- Forwarded message ----------
>>> From: Richard Wallis <richard.wallis at dataliberate.**com<richard.wallis at dataliberate.com>
>>> >
>>> Date: Fri, Aug 17, 2012 at 5:42 PM
>>> Subject: [LODLAM] Get Yourself a Linked Data Piece of WorldCat to Play
>>> With
>>> To: lod-lam at googlegroups.com
>>>
>>>
>>> In case you missed the press release earlier this week.
>>>
>>> You can now download a significant number of RDF triples describing
>>> the most highly held 1.2 million resources in WorldCat.  Licensed
>>> under ODC-BY.
>>>
>>> I've posted more details on my blog:
>>> http://dataliberate.com/2012/**08/get-yourself-a-linked-data-**
>>> piece-of-worldcat-to-play-**with/<http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-worldcat-to-play-with/>
>>>
>>> ~Richard
>>>
>>> --
>>>
>>>
>>>
>>>
>>>
>> --
>> Karen Coyle
>> kcoyle at kcoyle.net http://kcoyle.net
>> ph: 1-510-540-7596
>> m: 1-510-435-8234
>> skype: kcoylenet
>>
>>
>> ______________________________**_________________
>> open-bibliography mailing list
>> open-bibliography at lists.okfn.**org <open-bibliography at lists.okfn.org>
>> http://lists.okfn.org/mailman/**listinfo/open-bibliography<http://lists.okfn.org/mailman/listinfo/open-bibliography>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120817/3d6d229e/attachment.htm>

From kcoyle at kcoyle.net  Fri Aug 17 19:10:25 2012
From: kcoyle at kcoyle.net (Karen Coyle)
Date: Fri, 17 Aug 2012 11:10:25 -0700
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
 Piece of WorldCat to Play With
In-Reply-To: <CAJeHiNHJw-9JiX+=CU5YWSMunNJaBWB8htB6FhuBMvvSfwy5Fg@mail.gmail.com>
References: <CAD47Kz7kiBbweXRQ0-zE5u8roPKTG9fG-3FvRZmZRohcq1DEbg@mail.gmail.com>
	<CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>
	<502E75D1.1040803@kcoyle.net>
	<CAJcYwHaGXKjVQyCfDjp0FDHjdcRX9hWACkxDBZXgS6D6eMd2bA@mail.gmail.com>
	<CAJeHiNHJw-9JiX+=CU5YWSMunNJaBWB8htB6FhuBMvvSfwy5Fg@mail.gmail.com>
Message-ID: <502E8911.6000307@kcoyle.net>

Luc, I think this reflects an answer to your question. As with much that 
happens in computer technology, some of us have to depend on others. I 
find making our wishes clear helps guide those kind souls who have the 
necessary skills. Maybe we can work further with Tom and others to spell 
out what we need for this to be usable for us.

What we still need in the RDF world is the application that would do for 
the Semantic Web what Mosaic did for the Web: make it viewable and 
usable by the non-programmer. But first we have to have an actual 
Semantic Web, and I think that's still in progress in a strict sense.

kc

On 8/17/12 10:31 AM, Tom Johnson wrote:
> I'm in the process of putting up a triplestore w/ endpoint already. I
> have no problem sending out a link.
>
> I'm in an all day meeting today, so it might not happen until the weekend.
>
> On Fri, Aug 17, 2012 at 9:53 AM, Luc Gauvreau <lgovro at gmail.com
> <mailto:lgovro at gmail.com>> wrote:
>
>     Bonjour,
>
>     A very good question!
>
>     Multiple projects about linked datas and RDF, but who has the
>     expertise to use it?
>
>     Only experts and "geeks"?
>
>     Is it possible for an "amateur" to use these kind of format, files
>     and codes?
>
>     A kind of "Linked data and RDF for dummies" will be very usefull.
>     Merci,
>
>     Luc Gauvreau
>     (Montr?al, Qu?bec)
>
>
>
>     2012/8/17 Karen Coyle <kcoyle at kcoyle.net <mailto:kcoyle at kcoyle.net>>
>
>         I would love it if someone could put this in a triple store for
>         others to play with. How difficult is that?
>
>         kc
>
>
>         On 8/17/12 8:58 AM, Jonathan Gray wrote:
>
>             ---------- Forwarded message ----------
>             From: Richard Wallis <richard.wallis at dataliberate.__com
>             <mailto:richard.wallis at dataliberate.com>>
>             Date: Fri, Aug 17, 2012 at 5:42 PM
>             Subject: [LODLAM] Get Yourself a Linked Data Piece of
>             WorldCat to Play With
>             To: lod-lam at googlegroups.com <mailto:lod-lam at googlegroups.com>
>
>
>             In case you missed the press release earlier this week.
>
>             You can now download a significant number of RDF triples
>             describing
>             the most highly held 1.2 million resources in WorldCat.
>               Licensed
>             under ODC-BY.
>
>             I've posted more details on my blog:
>             http://dataliberate.com/2012/__08/get-yourself-a-linked-data-__piece-of-worldcat-to-play-__with/
>             <http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-worldcat-to-play-with/>
>
>             ~Richard
>
>             --
>
>
>
>
>
>         --
>         Karen Coyle
>         kcoyle at kcoyle.net <mailto:kcoyle at kcoyle.net> http://kcoyle.net
>         ph: 1-510-540-7596 <tel:1-510-540-7596>
>         m: 1-510-435-8234 <tel:1-510-435-8234>
>         skype: kcoylenet
>
>
>         _________________________________________________
>         open-bibliography mailing list
>         open-bibliography at lists.okfn.__org
>         <mailto:open-bibliography at lists.okfn.org>
>         http://lists.okfn.org/mailman/__listinfo/open-bibliography
>         <http://lists.okfn.org/mailman/listinfo/open-bibliography>
>
>
>

-- 
Karen Coyle
kcoyle at kcoyle.net http://kcoyle.net
ph: 1-510-540-7596
m: 1-510-435-8234
skype: kcoylenet


From pm286 at cam.ac.uk  Fri Aug 17 23:27:26 2012
From: pm286 at cam.ac.uk (Peter Murray-Rust)
Date: Fri, 17 Aug 2012 23:27:26 +0100
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
 Piece of WorldCat to Play With
In-Reply-To: <CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>
References: <CAD47Kz7kiBbweXRQ0-zE5u8roPKTG9fG-3FvRZmZRohcq1DEbg@mail.gmail.com>
	<CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>
Message-ID: <CAD2k14M7C0bUm7dh2iohZ4DHbKqCNwwuVFc3_69DjG_xApq7tQ@mail.gmail.com>

On Fri, Aug 17, 2012 at 4:58 PM, Jonathan Gray <jonathan.gray at okfn.org>wrote:

> ---------- Forwarded message ----------
> From: Richard Wallis <richard.wallis at dataliberate.com>
> Date: Fri, Aug 17, 2012 at 5:42 PM
> Subject: [LODLAM] Get Yourself a Linked Data Piece of WorldCat to Play With
> To: lod-lam at googlegroups.com
>
>
> In case you missed the press release earlier this week.
>
> You can now download a significant number of RDF triples describing
> the most highly held 1.2 million resources in WorldCat.  Licensed
> under ODC-BY.
>

Can anyone give an idea of what fraction this is - it sounds rather a small
number.

P.


>
> I've posted more details on my blog:
>
> http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-worldcat-to-play-with/
>
> ~Richard
>
> --
>
>
>
>
> --
> Jonathan Gray
>
> Head of Community
> The Open Knowledge Foundation
> http://www.okfn.org
>
> http://twitter.com/jwyg
>
> _______________________________________________
> open-bibliography mailing list
> open-bibliography at lists.okfn.org
> http://lists.okfn.org/mailman/listinfo/open-bibliography
>



-- 
Peter Murray-Rust
Reader in Molecular Informatics
Unilever Centre, Dep. Of Chemistry
University of Cambridge
CB2 1EW, UK
+44-1223-763069
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120817/c99c5325/attachment.htm>

From owen at ostephens.com  Fri Aug 17 23:43:42 2012
From: owen at ostephens.com (Owen Stephens)
Date: Fri, 17 Aug 2012 23:43:42 +0100
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
	Piece of WorldCat to Play With
In-Reply-To: <CAJcYwHaGXKjVQyCfDjp0FDHjdcRX9hWACkxDBZXgS6D6eMd2bA@mail.gmail.com>
References: <CAD47Kz7kiBbweXRQ0-zE5u8roPKTG9fG-3FvRZmZRohcq1DEbg@mail.gmail.com>
	<CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>
	<502E75D1.1040803@kcoyle.net>
	<CAJcYwHaGXKjVQyCfDjp0FDHjdcRX9hWACkxDBZXgS6D6eMd2bA@mail.gmail.com>
Message-ID: <5ABAA2E3-2BAB-43CC-897F-B0F5E847CEE8@ostephens.com>

On 17 Aug 2012, at 17:53, Luc Gauvreau <lgovro at gmail.com> wrote:

> A kind of "Linked data and RDF for dummies" will be very usefull.

Hi Luc,

I've written a few bits and pieces as I've learned about RDF and linked data that I hope might be useful:

http://www.meanboyfriend.com/overdue_ideas/2010/03/linked-data/
http://www.meanboyfriend.com/overdue_ideas/2010/04/whats-so-hard-about-linked-data/
and most recently http://www.meanboyfriend.com/overdue_ideas/2012/08/what-to-do-with-linked-data/

These are partly reflective pieces, and not intended as tutorials, but they do cover some of the underlying pieces and reflect my own journey learning about linked data so I hope they are some help.

I also wrote about my experiences querying RDF published by the British Museum using SPARQL: http://www.meanboyfriend.com/overdue_ideas/2011/12/experimenting-with-british-museum-data/. 

However, for a more structured introduction to RDF and SPARQL, specifically with bibliographic data in mind, I'd highly recommend this tutorial written by Ed Chamberlain at the University of Cambridge:
http://data.lib.cam.ac.uk/sparql.php

Hope some of these help

Owen
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120817/45699857/attachment-0001.htm>

From tfmorris at gmail.com  Fri Aug 17 23:43:47 2012
From: tfmorris at gmail.com (Tom Morris)
Date: Fri, 17 Aug 2012 18:43:47 -0400
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
 Piece of WorldCat to Play With
In-Reply-To: <CAD2k14M7C0bUm7dh2iohZ4DHbKqCNwwuVFc3_69DjG_xApq7tQ@mail.gmail.com>
References: <CAD47Kz7kiBbweXRQ0-zE5u8roPKTG9fG-3FvRZmZRohcq1DEbg@mail.gmail.com>
	<CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>
	<CAD2k14M7C0bUm7dh2iohZ4DHbKqCNwwuVFc3_69DjG_xApq7tQ@mail.gmail.com>
Message-ID: <CAE9vqEGpVB_iZ9C9mEaAJz90GX52mJ3BHAdODAyc+Arrjn=eTQ@mail.gmail.com>

On Fri, Aug 17, 2012 at 6:27 PM, Peter Murray-Rust <pm286 at cam.ac.uk> wrote:
> On Fri, Aug 17, 2012 at 4:58 PM, Jonathan Gray <jonathan.gray at okfn.org>
> wrote:
>>
>> ---------- Forwarded message ----------
>> From: Richard Wallis <richard.wallis at dataliberate.com>
>> Date: Fri, Aug 17, 2012 at 5:42 PM
>> Subject: [LODLAM] Get Yourself a Linked Data Piece of WorldCat to Play
>> With
>> To: lod-lam at googlegroups.com
>>
>>
>> In case you missed the press release earlier this week.
>>
>> You can now download a significant number of RDF triples describing
>> the most highly held 1.2 million resources in WorldCat.  Licensed
>> under ODC-BY.
>
>
> Can anyone give an idea of what fraction this is - it sounds rather a small
> number.

1.2/271 or <<1%

http://www.oclc.org/worldcat/statistics/default.htm

Tom


From tfmorris at gmail.com  Fri Aug 17 23:50:33 2012
From: tfmorris at gmail.com (Tom Morris)
Date: Fri, 17 Aug 2012 18:50:33 -0400
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
 Piece of WorldCat to Play With
In-Reply-To: <502E8911.6000307@kcoyle.net>
References: <CAD47Kz7kiBbweXRQ0-zE5u8roPKTG9fG-3FvRZmZRohcq1DEbg@mail.gmail.com>
	<CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>
	<502E75D1.1040803@kcoyle.net>
	<CAJcYwHaGXKjVQyCfDjp0FDHjdcRX9hWACkxDBZXgS6D6eMd2bA@mail.gmail.com>
	<CAJeHiNHJw-9JiX+=CU5YWSMunNJaBWB8htB6FhuBMvvSfwy5Fg@mail.gmail.com>
	<502E8911.6000307@kcoyle.net>
Message-ID: <CAE9vqEHqbPryCNB0yb-XBL3ki+zbGcvLmdYLmofPjfs-tegTqA@mail.gmail.com>

Karen,

On Fri, Aug 17, 2012 at 2:10 PM, Karen Coyle <kcoyle at kcoyle.net> wrote:
> Luc, I think this reflects an answer to your question. As with much that
> happens in computer technology, some of us have to depend on others. I find
> making our wishes clear helps guide those kind souls who have the necessary
> skills. Maybe we can work further with Tom and others to spell out what we
> need for this to be usable for us.

Does having this data loaded into a triple store help you?  What types
of things would it enable?

It seems like it might be marginally better than a raw RDF file, but
it seems like it would still take a fair amount of work to do anything
useful with it.

Tom

p.s. I'm curious to see if the other Tom is able to load it using his
tools because it looks to me like it contains invalid URIs (embedded
spaces) which may cause RDF tools to choke depending on how picky they
are about parsing.

>
> What we still need in the RDF world is the application that would do for the
> Semantic Web what Mosaic did for the Web: make it viewable and usable by the
> non-programmer. But first we have to have an actual Semantic Web, and I
> think that's still in progress in a strict sense.
>
> kc
>
>
> On 8/17/12 10:31 AM, Tom Johnson wrote:
>>
>> I'm in the process of putting up a triplestore w/ endpoint already. I
>> have no problem sending out a link.
>>
>> I'm in an all day meeting today, so it might not happen until the weekend.
>>
>> On Fri, Aug 17, 2012 at 9:53 AM, Luc Gauvreau <lgovro at gmail.com
>> <mailto:lgovro at gmail.com>> wrote:
>>
>>     Bonjour,
>>
>>     A very good question!
>>
>>     Multiple projects about linked datas and RDF, but who has the
>>     expertise to use it?
>>
>>     Only experts and "geeks"?
>>
>>     Is it possible for an "amateur" to use these kind of format, files
>>     and codes?
>>
>>     A kind of "Linked data and RDF for dummies" will be very usefull.
>>     Merci,
>>
>>     Luc Gauvreau
>>     (Montr?al, Qu?bec)
>>
>>
>>
>>     2012/8/17 Karen Coyle <kcoyle at kcoyle.net <mailto:kcoyle at kcoyle.net>>
>>
>>
>>         I would love it if someone could put this in a triple store for
>>         others to play with. How difficult is that?
>>
>>         kc
>>
>>
>>         On 8/17/12 8:58 AM, Jonathan Gray wrote:
>>
>>             ---------- Forwarded message ----------
>>             From: Richard Wallis <richard.wallis at dataliberate.__com
>>             <mailto:richard.wallis at dataliberate.com>>
>>             Date: Fri, Aug 17, 2012 at 5:42 PM
>>             Subject: [LODLAM] Get Yourself a Linked Data Piece of
>>             WorldCat to Play With
>>             To: lod-lam at googlegroups.com <mailto:lod-lam at googlegroups.com>
>>
>>
>>             In case you missed the press release earlier this week.
>>
>>             You can now download a significant number of RDF triples
>>             describing
>>             the most highly held 1.2 million resources in WorldCat.
>>               Licensed
>>             under ODC-BY.
>>
>>             I've posted more details on my blog:
>>
>> http://dataliberate.com/2012/__08/get-yourself-a-linked-data-__piece-of-worldcat-to-play-__with/
>>
>>
>> <http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-worldcat-to-play-with/>
>>
>>             ~Richard
>>


From kcoyle at kcoyle.net  Sat Aug 18 00:11:36 2012
From: kcoyle at kcoyle.net (Karen Coyle)
Date: Fri, 17 Aug 2012 16:11:36 -0700
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
 Piece of WorldCat to Play With
In-Reply-To: <CAD2k14M7C0bUm7dh2iohZ4DHbKqCNwwuVFc3_69DjG_xApq7tQ@mail.gmail.com>
References: <CAD47Kz7kiBbweXRQ0-zE5u8roPKTG9fG-3FvRZmZRohcq1DEbg@mail.gmail.com>
	<CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>
	<CAD2k14M7C0bUm7dh2iohZ4DHbKqCNwwuVFc3_69DjG_xApq7tQ@mail.gmail.com>
Message-ID: <502ECFA8.2050408@kcoyle.net>

Peter, it is a small number, but the fact that it is the top 1.2 million 
in terms of library holdings is significant. Think of this as the set of 
the most popular items from WorldCat -- so as a set for experimentation, 
most libraries will find something in their collection that can link to 
this dataset. (It consists of all items held by >=250 libraries).

OCLC's statistics show about 185 million book records -- but there will 
be some duplication in that.

kc

On 8/17/12 3:27 PM, Peter Murray-Rust wrote:
>
>
> On Fri, Aug 17, 2012 at 4:58 PM, Jonathan Gray <jonathan.gray at okfn.org
> <mailto:jonathan.gray at okfn.org>> wrote:
>
>     ---------- Forwarded message ----------
>     From: Richard Wallis <richard.wallis at dataliberate.com
>     <mailto:richard.wallis at dataliberate.com>>
>     Date: Fri, Aug 17, 2012 at 5:42 PM
>     Subject: [LODLAM] Get Yourself a Linked Data Piece of WorldCat to
>     Play With
>     To: lod-lam at googlegroups.com <mailto:lod-lam at googlegroups.com>
>
>
>     In case you missed the press release earlier this week.
>
>     You can now download a significant number of RDF triples describing
>     the most highly held 1.2 million resources in WorldCat.  Licensed
>     under ODC-BY.
>
> Can anyone give an idea of what fraction this is - it sounds rather a
> small number.
> P.
>
>
>     I've posted more details on my blog:
>     http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-worldcat-to-play-with/
>
>     ~Richard
>
>     --
>
>
>
>
>     --
>     Jonathan Gray
>
>     Head of Community
>     The Open Knowledge Foundation
>     http://www.okfn.org
>
>     http://twitter.com/jwyg
>
>     _______________________________________________
>     open-bibliography mailing list
>     open-bibliography at lists.okfn.org
>     <mailto:open-bibliography at lists.okfn.org>
>     http://lists.okfn.org/mailman/listinfo/open-bibliography
>
>
>
>
> --
> Peter Murray-Rust
> Reader in Molecular Informatics
> Unilever Centre, Dep. Of Chemistry
> University of Cambridge
> CB2 1EW, UK
> +44-1223-763069
>
>
> _______________________________________________
> open-bibliography mailing list
> open-bibliography at lists.okfn.org
> http://lists.okfn.org/mailman/listinfo/open-bibliography
>

-- 
Karen Coyle
kcoyle at kcoyle.net http://kcoyle.net
ph: 1-510-540-7596
m: 1-510-435-8234
skype: kcoylenet


From thomas.johnson at oregonstate.edu  Sat Aug 18 00:14:39 2012
From: thomas.johnson at oregonstate.edu (Tom Johnson)
Date: Fri, 17 Aug 2012 16:14:39 -0700
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
 Piece of WorldCat to Play With
In-Reply-To: <CAE9vqEHqbPryCNB0yb-XBL3ki+zbGcvLmdYLmofPjfs-tegTqA@mail.gmail.com>
References: <CAD47Kz7kiBbweXRQ0-zE5u8roPKTG9fG-3FvRZmZRohcq1DEbg@mail.gmail.com>
	<CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>
	<502E75D1.1040803@kcoyle.net>
	<CAJcYwHaGXKjVQyCfDjp0FDHjdcRX9hWACkxDBZXgS6D6eMd2bA@mail.gmail.com>
	<CAJeHiNHJw-9JiX+=CU5YWSMunNJaBWB8htB6FhuBMvvSfwy5Fg@mail.gmail.com>
	<502E8911.6000307@kcoyle.net>
	<CAE9vqEHqbPryCNB0yb-XBL3ki+zbGcvLmdYLmofPjfs-tegTqA@mail.gmail.com>
Message-ID: <CAJeHiNEfC_W=KQK74o=oqz6-ffz+qhNgD5cRvWnY8A-1g+SnxQ@mail.gmail.com>

I'm not having any trouble loading it (except that it is slow). I'm fussing
with the best way to configure 4store to handle ~80 million triples.

The data looks good to me.

I'm also putting up a pubby-like front end.

I'm not sure what the real cost of running a SPARQL endpoint for a dataset
like this is going to look like, or whether I can support it in the long
run. Still, I'm interested in hearing what people would want to see and how
they would use it if I (or Oregon State) were to run services on it.

- Tom

On Fri, Aug 17, 2012 at 3:50 PM, Tom Morris <tfmorris at gmail.com> wrote:

> Karen,
>
> On Fri, Aug 17, 2012 at 2:10 PM, Karen Coyle <kcoyle at kcoyle.net> wrote:
> > Luc, I think this reflects an answer to your question. As with much that
> > happens in computer technology, some of us have to depend on others. I
> find
> > making our wishes clear helps guide those kind souls who have the
> necessary
> > skills. Maybe we can work further with Tom and others to spell out what
> we
> > need for this to be usable for us.
>
> Does having this data loaded into a triple store help you?  What types
> of things would it enable?
>
> It seems like it might be marginally better than a raw RDF file, but
> it seems like it would still take a fair amount of work to do anything
> useful with it.
>
> Tom
>
> p.s. I'm curious to see if the other Tom is able to load it using his
> tools because it looks to me like it contains invalid URIs (embedded
> spaces) which may cause RDF tools to choke depending on how picky they
> are about parsing.
>
> >
> > What we still need in the RDF world is the application that would do for
> the
> > Semantic Web what Mosaic did for the Web: make it viewable and usable by
> the
> > non-programmer. But first we have to have an actual Semantic Web, and I
> > think that's still in progress in a strict sense.
> >
> > kc
> >
> >
> > On 8/17/12 10:31 AM, Tom Johnson wrote:
> >>
> >> I'm in the process of putting up a triplestore w/ endpoint already. I
> >> have no problem sending out a link.
> >>
> >> I'm in an all day meeting today, so it might not happen until the
> weekend.
> >>
> >> On Fri, Aug 17, 2012 at 9:53 AM, Luc Gauvreau <lgovro at gmail.com
> >> <mailto:lgovro at gmail.com>> wrote:
> >>
> >>     Bonjour,
> >>
> >>     A very good question!
> >>
> >>     Multiple projects about linked datas and RDF, but who has the
> >>     expertise to use it?
> >>
> >>     Only experts and "geeks"?
> >>
> >>     Is it possible for an "amateur" to use these kind of format, files
> >>     and codes?
> >>
> >>     A kind of "Linked data and RDF for dummies" will be very usefull.
> >>     Merci,
> >>
> >>     Luc Gauvreau
> >>     (Montr?al, Qu?bec)
> >>
> >>
> >>
> >>     2012/8/17 Karen Coyle <kcoyle at kcoyle.net <mailto:kcoyle at kcoyle.net
> >>
> >>
> >>
> >>         I would love it if someone could put this in a triple store for
> >>         others to play with. How difficult is that?
> >>
> >>         kc
> >>
> >>
> >>         On 8/17/12 8:58 AM, Jonathan Gray wrote:
> >>
> >>             ---------- Forwarded message ----------
> >>             From: Richard Wallis <richard.wallis at dataliberate.__com
> >>             <mailto:richard.wallis at dataliberate.com>>
> >>             Date: Fri, Aug 17, 2012 at 5:42 PM
> >>             Subject: [LODLAM] Get Yourself a Linked Data Piece of
> >>             WorldCat to Play With
> >>             To: lod-lam at googlegroups.com <mailto:
> lod-lam at googlegroups.com>
> >>
> >>
> >>             In case you missed the press release earlier this week.
> >>
> >>             You can now download a significant number of RDF triples
> >>             describing
> >>             the most highly held 1.2 million resources in WorldCat.
> >>               Licensed
> >>             under ODC-BY.
> >>
> >>             I've posted more details on my blog:
> >>
> >>
> http://dataliberate.com/2012/__08/get-yourself-a-linked-data-__piece-of-worldcat-to-play-__with/
> >>
> >>
> >> <
> http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-worldcat-to-play-with/
> >
> >>
> >>             ~Richard
> >>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120817/2745b624/attachment.htm>

From pm286 at cam.ac.uk  Sat Aug 18 00:30:33 2012
From: pm286 at cam.ac.uk (Peter Murray-Rust)
Date: Sat, 18 Aug 2012 00:30:33 +0100
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
 Piece of WorldCat to Play With
In-Reply-To: <502ECFA8.2050408@kcoyle.net>
References: <CAD47Kz7kiBbweXRQ0-zE5u8roPKTG9fG-3FvRZmZRohcq1DEbg@mail.gmail.com>
	<CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>
	<CAD2k14M7C0bUm7dh2iohZ4DHbKqCNwwuVFc3_69DjG_xApq7tQ@mail.gmail.com>
	<502ECFA8.2050408@kcoyle.net>
Message-ID: <CAD2k14O8Z5nfn_7SmwgtNE-ZSbCg3RWO7JGEOorD0cT5FCdGzA@mail.gmail.com>

On Sat, Aug 18, 2012 at 12:11 AM, Karen Coyle <kcoyle at kcoyle.net> wrote:

> Peter, it is a small number, but the fact that it is the top 1.2 million
> in terms of library holdings is significant. Think of this as the set of
> the most popular items from WorldCat -- so as a set for experimentation,
> most libraries will find something in their collection that can link to
> this dataset. (It consists of all items held by >=250 libraries).
>
> I suspect it is considerably smaller than the previous releases by the
European national libraries. If there is a real intention to release the
185-million others and a timescale then it's valuable. If it's a small part
- even though it's the most popular - then the primary use is simply
exploring OCLC's RDF technology - and like all RDFs it's probably pretty
idiosyncratic. My rough guess on the 1% + power law would be that someone
searching for a book would still fail 90+% of the time.



-- 
Peter Murray-Rust
Reader in Molecular Informatics
Unilever Centre, Dep. Of Chemistry
University of Cambridge
CB2 1EW, UK
+44-1223-763069
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120818/84945961/attachment-0001.htm>

From kcoyle at kcoyle.net  Sat Aug 18 05:16:16 2012
From: kcoyle at kcoyle.net (Karen Coyle)
Date: Fri, 17 Aug 2012 21:16:16 -0700
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
 Piece of WorldCat to Play With
In-Reply-To: <CAJeHiNEfC_W=KQK74o=oqz6-ffz+qhNgD5cRvWnY8A-1g+SnxQ@mail.gmail.com>
References: <CAD47Kz7kiBbweXRQ0-zE5u8roPKTG9fG-3FvRZmZRohcq1DEbg@mail.gmail.com>
	<CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>
	<502E75D1.1040803@kcoyle.net>
	<CAJcYwHaGXKjVQyCfDjp0FDHjdcRX9hWACkxDBZXgS6D6eMd2bA@mail.gmail.com>
	<CAJeHiNHJw-9JiX+=CU5YWSMunNJaBWB8htB6FhuBMvvSfwy5Fg@mail.gmail.com>
	<502E8911.6000307@kcoyle.net>
	<CAE9vqEHqbPryCNB0yb-XBL3ki+zbGcvLmdYLmofPjfs-tegTqA@mail.gmail.com>
	<CAJeHiNEfC_W=KQK74o=oqz6-ffz+qhNgD5cRvWnY8A-1g+SnxQ@mail.gmail.com>
Message-ID: <502F1710.5070503@kcoyle.net>

Tom,

I'll poke around and see if anyone is using any "easy" visualization 
software. The Nat'l Lib. of Spain did some neat stuff with Graphviz, but 
I have no idea what that took. [1]

For basic functionality, I'd love to see a minimal web form that would 
launch a search via SPARQL. (Pubby may do this -- I don't see a screen 
shot that answers this for me.) It would probably have to be limited to 
searching on only certain values, but that's ok for a start. At minimum, 
pulling up everything with the subject URI of an OCLC number or an 
object with the VIAF URI. Since the URI patterns for those are set, it 
should be possible to have a form for just the number and to fill in the 
full URI for the search. An even easier alternative would be to supply 
the SPARQL patterns for those searches, to be launched from within a web 
page. I could find examples of what I mean if this isn't clear. In any 
case, being able to do some minimal searching seems to be a best first step.

Thanks,

kc
[1] http://bne.linkeddata.es/graphvis/

On 8/17/12 4:14 PM, Tom Johnson wrote:
> I'm not having any trouble loading it (except that it is slow). I'm
> fussing with the best way to configure 4store to handle ~80 million triples.
>
> The data looks good to me.
>
> I'm also putting up a pubby-like front end.
>
> I'm not sure what the real cost of running a SPARQL endpoint for a
> dataset like this is going to look like, or whether I can support it in
> the long run. Still, I'm interested in hearing what people would want to
> see and how they would use it if I (or Oregon State) were to run
> services on it.
>
> - Tom
>
> On Fri, Aug 17, 2012 at 3:50 PM, Tom Morris <tfmorris at gmail.com
> <mailto:tfmorris at gmail.com>> wrote:
>
>     Karen,
>
>     On Fri, Aug 17, 2012 at 2:10 PM, Karen Coyle <kcoyle at kcoyle.net
>     <mailto:kcoyle at kcoyle.net>> wrote:
>      > Luc, I think this reflects an answer to your question. As with
>     much that
>      > happens in computer technology, some of us have to depend on
>     others. I find
>      > making our wishes clear helps guide those kind souls who have the
>     necessary
>      > skills. Maybe we can work further with Tom and others to spell
>     out what we
>      > need for this to be usable for us.
>
>     Does having this data loaded into a triple store help you?  What types
>     of things would it enable?
>
>     It seems like it might be marginally better than a raw RDF file, but
>     it seems like it would still take a fair amount of work to do anything
>     useful with it.
>
>     Tom
>
>     p.s. I'm curious to see if the other Tom is able to load it using his
>     tools because it looks to me like it contains invalid URIs (embedded
>     spaces) which may cause RDF tools to choke depending on how picky they
>     are about parsing.
>
>      >
>      > What we still need in the RDF world is the application that would
>     do for the
>      > Semantic Web what Mosaic did for the Web: make it viewable and
>     usable by the
>      > non-programmer. But first we have to have an actual Semantic Web,
>     and I
>      > think that's still in progress in a strict sense.
>      >
>      > kc
>      >
>      >
>      > On 8/17/12 10:31 AM, Tom Johnson wrote:
>      >>
>      >> I'm in the process of putting up a triplestore w/ endpoint
>     already. I
>      >> have no problem sending out a link.
>      >>
>      >> I'm in an all day meeting today, so it might not happen until
>     the weekend.
>      >>
>      >> On Fri, Aug 17, 2012 at 9:53 AM, Luc Gauvreau <lgovro at gmail.com
>     <mailto:lgovro at gmail.com>
>      >> <mailto:lgovro at gmail.com <mailto:lgovro at gmail.com>>> wrote:
>      >>
>      >>     Bonjour,
>      >>
>      >>     A very good question!
>      >>
>      >>     Multiple projects about linked datas and RDF, but who has the
>      >>     expertise to use it?
>      >>
>      >>     Only experts and "geeks"?
>      >>
>      >>     Is it possible for an "amateur" to use these kind of format,
>     files
>      >>     and codes?
>      >>
>      >>     A kind of "Linked data and RDF for dummies" will be very
>     usefull.
>      >>     Merci,
>      >>
>      >>     Luc Gauvreau
>      >>     (Montr?al, Qu?bec)
>      >>
>      >>
>      >>
>      >>     2012/8/17 Karen Coyle <kcoyle at kcoyle.net
>     <mailto:kcoyle at kcoyle.net> <mailto:kcoyle at kcoyle.net
>     <mailto:kcoyle at kcoyle.net>>>
>      >>
>      >>
>      >>         I would love it if someone could put this in a triple
>     store for
>      >>         others to play with. How difficult is that?
>      >>
>      >>         kc
>      >>
>      >>
>      >>         On 8/17/12 8:58 AM, Jonathan Gray wrote:
>      >>
>      >>             ---------- Forwarded message ----------
>      >>             From: Richard Wallis <richard.wallis at dataliberate.__com
>      >>             <mailto:richard.wallis at dataliberate.com
>     <mailto:richard.wallis at dataliberate.com>>>
>      >>             Date: Fri, Aug 17, 2012 at 5:42 PM
>      >>             Subject: [LODLAM] Get Yourself a Linked Data Piece of
>      >>             WorldCat to Play With
>      >>             To: lod-lam at googlegroups.com
>     <mailto:lod-lam at googlegroups.com> <mailto:lod-lam at googlegroups.com
>     <mailto:lod-lam at googlegroups.com>>
>      >>
>      >>
>      >>             In case you missed the press release earlier this week.
>      >>
>      >>             You can now download a significant number of RDF triples
>      >>             describing
>      >>             the most highly held 1.2 million resources in WorldCat.
>      >>               Licensed
>      >>             under ODC-BY.
>      >>
>      >>             I've posted more details on my blog:
>      >>
>      >>
>     http://dataliberate.com/2012/__08/get-yourself-a-linked-data-__piece-of-worldcat-to-play-__with/
>      >>
>      >>
>      >>
>     <http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-worldcat-to-play-with/>
>      >>
>      >>             ~Richard
>      >>
>
>

-- 
Karen Coyle
kcoyle at kcoyle.net http://kcoyle.net
ph: 1-510-540-7596
m: 1-510-435-8234
skype: kcoylenet


From jcmcoppice12 at gmail.com  Sat Aug 18 11:38:56 2012
From: jcmcoppice12 at gmail.com (Jenny Molloy)
Date: Sat, 18 Aug 2012 11:38:56 +0100
Subject: [open-bibliography] [open-science-dev] Fwd: [open-science] fw:
 Python NLTK/data mining/machine learning project of public research data,
 anyone interested?
In-Reply-To: <CAGS_vfpKyoRTpAtWXamgJhAPtRt-RD=3ZYdroC4jy8b17cTc9w@mail.gmail.com>
References: <mailman.17.1345197601.15227.trizpug@python.org>
	<87has1n7sc.fsf@pobox.com>
	<CABPM+woF6+ytXYCtYp07P2feFgwutSpkTgK-59+ckoEeYHhYmw@mail.gmail.com>
	<CAGS_vfpKyoRTpAtWXamgJhAPtRt-RD=3ZYdroC4jy8b17cTc9w@mail.gmail.com>
Message-ID: <CABPM+wr+Rs1V2e6aBtC=-Wo74aVpmtG-RvPH49CKYe5ooQ27+g@mail.gmail.com>

Thanks Daniel!

The original email was from Nathan Rice who I've cc'd into the thread, if
you could reply to him.

Nathan - Tom forwarded your email to the Open Knowledge Foundation lists,
we have some people interested in text and data mining on board so
hopefully they'll be able to offer some advice/assistance! We run periodic
hack days, in the UK mostly, but one coming up in Helsinki on 18 September
if you'd be interested to do a remote demo, at the last one there was some
work on content mining phylogenetic trees
http://rossmounce.co.uk/2012/07/17/content-mining-for-phylogenetic-data/

Jenny

On Fri, Aug 17, 2012 at 1:39 PM, Daniel Lombra?a Gonz?lez <
teleyinex at gmail.com> wrote:

> Hi,
>
> I think this project could be interesting for PyBossa in the sense that
> some data-mining and validation could be done by humans :-) I can give Tom
> more details if PyBossa is helpful :-)
>
> Cheers,
>
> Daniel
>
> On Fri, Aug 17, 2012 at 2:09 PM, Jenny Molloy <jcmcoppice12 at gmail.com>wrote:
>
>> Hi All
>>
>> Apologies for cross-posting but this came out on open-science and I
>> thought it might be of interest to some of you as well
>>
>> Jenny
>>
>> ---------- Forwarded message ----------
>> From: Tom Roche <Tom_Roche at pobox.com>
>> Date: Fri, Aug 17, 2012 at 11:15 AM
>> Subject: [open-science] fw: Python NLTK/data mining/machine learning
>> project of public research data, anyone interested?
>> To: open-science at lists.okfn.org
>>
>>
>>
>> Dunno if the following is OT for this group, but thought this thread
>> from the local PUG might be of interest. (Note I don't know the
>> author personally; reply to him, not me.)
>>
>> http://mail.python.org/pipermail/trizpug/2012-August/001919.html
>> > Nathan Rice nathan.alexander.rice at gmail.com
>> > Thu Aug 16 20:31:00 CEST 2012
>>
>> > Hi All,
>>
>> > Normally, my projects are pretty boring, and I prefer to endure the
>> > suffering in solitary silence. As luck would have it though, I
>> > actually have an interesting project on my plate currently, and I
>> > think it is cool enough that I wanted to give other people the
>> > opportunity to stick their noses in and provide input or play with
>> > some code.
>>
>> > I am currently involved in compiling a database of medical data
>> > (published clinical or pre-clinical trials) surrounding ethno- and
>> > alternative- medicinal treatments, for semi-automated meta analysis
>> > and treatment guidance. In order for this to work, a lot of
>> > technical challenges have to be overcome:
>>
>> > My initial tally from PubMed puts the number of articles at over
>> > 70,000; based on visual inspection, many of these are not actually
>> > applicable, but there are limited filtering options via the Entrez
>> > web API. Machine learning techniques would probably be very helpful
>> > at scoring articles for applicability, and ignoring ones that are
>> > clearly inapplicable.
>>
>> > In order to perform meta-analysis and treatment guidance, the
>> > article needs to be mined for treatment, condition, circumstances of
>> > treatment and condition, and whether it was successful or not (with
>> > some p value and sample size). Most of this is not available as
>> > standard metadata for the studies, and must be mined from the text.
>>
>> > In addition, not all studies are equal. Methodological errors, lack
>> > of reproduciblity, and so forth can all render a study meaningless.
>> > Thus, studies must have a scoring mechanism so you can avoid
>> > tainting meta-analyses with biased data. This scoring mechanism will
>> > probably include the impact factor of the journal, the g/h-index of
>> > the authors, the number of (non self) citations, etc.
>>
>> > As you can see, each of these is meaty, and all of them need to be
>> > taken care of to get good results :) If anyone is interested in
>> > getting some serious natural language processing/data mining/machine
>> > learning practice, I'd love to involve you. There's no reason I
>> > should have all the fun!
>>
>> http://mail.python.org/pipermail/trizpug/2012-August/001920.html
>> > I'm still in the planning stages for most of the stuff; I have the
>> > pubmed extraction code pretty well nailed, and I have a solid
>> > outline for the article disqualification (create a feature vector
>> > out of topic and abstract bigrams, MeSH subject headings and
>> > journal, use a SVM discriminator and manually generate a RoC curve
>> > to determine the cutoff score) but I'm still very up in the air
>> > regarding NL extraction of things like sample size, significance,
>> > etc. If you'd like to learn more I would of course be happy to go
>> > over my thoughts on the matter and we can play around with some
>> > code.
>>
>> _______________________________________________
>> open-science mailing list
>> open-science at lists.okfn.org
>> http://lists.okfn.org/mailman/listinfo/open-science
>>
>>
>> _______________________________________________
>> open-science-dev mailing list
>> open-science-dev at lists.okfn.org
>> http://lists.okfn.org/mailman/listinfo/open-science-dev
>>
>>
>
>
> --
>
> ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
> http://github.com/teleyinex
> http://www.flickr.com/photos/teleyinex
>
> ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
> Por favor, NO utilice formatos de archivo propietarios para el
> intercambio de documentos, como DOC y XLS, sino PDF, HTML, RTF, TXT, CSV
> o cualquier otro que no obligue a utilizar un programa de un
> fabricante concreto para tratar la informaci?n contenida en ?l.
>
> ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120818/d4c389e6/attachment.htm>

From jenny.molloy at okfn.org  Sat Aug 18 11:46:19 2012
From: jenny.molloy at okfn.org (Jenny Molloy)
Date: Sat, 18 Aug 2012 11:46:19 +0100
Subject: [open-bibliography] Call for Participation: Digital Research 2012,
	10-12 September, Oxford
Message-ID: <CABPM+wq-ZBDTjwuZiHr93Og22O7Xn3Z8HiJDez47G5L7X-Sk6A@mail.gmail.com>

                    Digital Research 2012

                    10-12 September 2012
             St Catherine?s College, Oxford, UK

                digital-research.oerc.ox.ac.uk

 Digital Research 2012 features an exciting 3-day programme with a great
set of invited speakers together with showcases of the work and vision
of the Digital Research community.  Here are some highlights of the
programme - please see the website for the full programme and
registration information:

 * New Science of New Data Symposium and Innovation Showcase (Mon 10th).
Keynotes from Noshir Contractor (Northwestern University) on Web
Science, Nigel Shadbolt (Government Information Adviser) on Open Data
and a closing address by Kieron O'Hara (philosopher, computer scientist
and political writer) - with twitter analytics, geolocated social media
and web observatories in between.  Also the launch of the Software
Sustainability Institute's Fellows programme and community workshops.

 * Future of Digital Research (Tue 11th).  Keynotes from Stevan Harnad on
"Digital Research: How and Why the RCUK Open Access Policy Needs to
Be Revised", Jim Hendler (Rensselaer Polytechnic Institute) on "Broad
Data" (not just big!), and Lizbeth Goodman (UCD) on "SMART spaces by
And for SMART people". Sessions are themed on Open Science, Smart
Spaces as a Utility and future glimpses from the community, culminating
in a Roundtable discussion on the Future of Digital Research.

 * e?Infrastructure Forum and Innovation Showcase (Wed 12th).  We
open with a dual-track community innovation showcase, then launch the
UK e-Infrastructure Academic Community Forum where Peter Coveney
(UK e-Infrastructure Leadership Council and UCL) will present the "state
of the nation" followed by a Provider?s Panel, Software, Training and
User?s Panel - an important and timely opportunity for the community to
review where we are and determine what's needed in the future.

 There's a lot more happening throughout the event, including an exciting
"DevChallenge" hackathon run by DevCSI, software surgery by SSI and
multiple community workshops - plus the Digital Research 2012 dinner in
College and a reception in the spectacular Museum of Natural History.
We're very grateful to everyone who has come together to make this event
possible, including e-Research South, Open Knowledge Foundation, Web
Science, the Digital Social Research programme, our Digital Economy
colleagues and the All Hands foundation.

 We look forward to seeing you at Digital Research 2012 in Oxford in
September.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120818/269d51a6/attachment-0001.htm>

From pm286 at cam.ac.uk  Sat Aug 18 16:50:18 2012
From: pm286 at cam.ac.uk (Peter Murray-Rust)
Date: Sat, 18 Aug 2012 16:50:18 +0100
Subject: [open-bibliography] [Open-access] [open-science-dev] Fwd:
 [open-science] fw: Python NLTK/data mining/machine learning project of
 public research data, anyone interested?
In-Reply-To: <CABPM+wr+Rs1V2e6aBtC=-Wo74aVpmtG-RvPH49CKYe5ooQ27+g@mail.gmail.com>
References: <mailman.17.1345197601.15227.trizpug@python.org>
	<87has1n7sc.fsf@pobox.com>
	<CABPM+woF6+ytXYCtYp07P2feFgwutSpkTgK-59+ckoEeYHhYmw@mail.gmail.com>
	<CAGS_vfpKyoRTpAtWXamgJhAPtRt-RD=3ZYdroC4jy8b17cTc9w@mail.gmail.com>
	<CABPM+wr+Rs1V2e6aBtC=-Wo74aVpmtG-RvPH49CKYe5ooQ27+g@mail.gmail.com>
Message-ID: <CAD2k14MZsPquK2c=+0SDqubhjBA+wQPOTr2q1j+z-FB56hAQ0A@mail.gmail.com>

On Sat, Aug 18, 2012 at 11:38 AM, Jenny Molloy <jcmcoppice12 at gmail.com>wrote:

> Thanks Daniel!
>
> The original email was from Nathan Rice who I've cc'd into the thread, if
> you could reply to him.
>
> Nathan - Tom forwarded your email to the Open Knowledge Foundation lists,
> we have some people interested in text and data mining on board so
> hopefully they'll be able to offer some advice/assistance! We run periodic
> hack days, in the UK mostly, but one coming up in Helsinki on 18 September
> if you'd be interested to do a remote demo, at the last one there was some
> work on content mining phylogenetic trees
> http://rossmounce.co.uk/2012/07/17/content-mining-for-phylogenetic-data/
>
>
This is really exciting. Be aware, Nathan, that if you come up with a good
idea (which you have) you are likely to end up making it work!

I am copying the three lists because this is still very general but we
should condense it later to more specialist topics.

There is a real need for coordinating content-mining. There are a  number
of threads:
* tools to systematically retrieve "documents", such as PubCrawler whcih
crawls publishers' sites. At some stage we shall press the button and
collect a large amount of metadata. This distributes very nicely with one
scraper per publisher (about 100 in science according to Ross Mounce). We
have scrapers for some of the major ones but will need this for others.
First one required is BMC (which is ultra-legal as it's CC-BY)
* protocols for content-mining - legal and technical
* collation of experiences and groups in content-mining. This is really
important. I've been hacking in this area for 3 months and I am not
connected with other efforts nor they with me (apart from about 3 bio-
ones). We need to bring content-miners together. Since a major barrier is
publisher FUD let's give each other confidence. Also the OKF's Datahub is a
great place to put Open results.
* collation of technologies. There are at least:
  - scrapers
  - PDF hacking (I have done a lot of this but we need more. Open font
info. Postscript reconstruction
  - bit-map hacking for diagram analysis (I am excited by the quality of
modern scientific diagrams and think there can be a lot of automation)
  - shallow natural language processing and NPL resources (e.g. vocabs,
character sets)
  - classification techniques (e.g. Lucene/Solr) for text and diagrams

I think if we harness all these we will have a large step change in the
automation of extraction of scientific information from "the literature".

And one-by-one the publishers will come to us because they will need us.

Timescale - about 1 year to have something major to report - about 5 years
to change the way scientific information is managed.

> Jenny
>
> On Fri, Aug 17, 2012 at 1:39 PM, Daniel Lombra?a Gonz?lez <
> teleyinex at gmail.com> wrote:
>
>> Hi,
>>
>> I think this project could be interesting for PyBossa in the sense that
>> some data-mining and validation could be done by humans :-) I can give Tom
>> more details if PyBossa is helpful :-)
>>
>> Cheers,
>>
>> Daniel
>>
>> On Fri, Aug 17, 2012 at 2:09 PM, Jenny Molloy <jcmcoppice12 at gmail.com>wrote:
>>
>>> Hi All
>>>
>>> Apologies for cross-posting but this came out on open-science and I
>>> thought it might be of interest to some of you as well
>>>
>>> Jenny
>>>
>>> ---------- Forwarded message ----------
>>> From: Tom Roche <Tom_Roche at pobox.com>
>>> Date: Fri, Aug 17, 2012 at 11:15 AM
>>> Subject: [open-science] fw: Python NLTK/data mining/machine learning
>>> project of public research data, anyone interested?
>>> To: open-science at lists.okfn.org
>>>
>>>
>>>
>>> Dunno if the following is OT for this group, but thought this thread
>>> from the local PUG might be of interest. (Note I don't know the
>>> author personally; reply to him, not me.)
>>>
>>> http://mail.python.org/pipermail/trizpug/2012-August/001919.html
>>> > Nathan Rice nathan.alexander.rice at gmail.com
>>> > Thu Aug 16 20:31:00 CEST 2012
>>>
>>> > Hi All,
>>>
>>> > Normally, my projects are pretty boring, and I prefer to endure the
>>> > suffering in solitary silence. As luck would have it though, I
>>> > actually have an interesting project on my plate currently, and I
>>> > think it is cool enough that I wanted to give other people the
>>> > opportunity to stick their noses in and provide input or play with
>>> > some code.
>>>
>>> > I am currently involved in compiling a database of medical data
>>> > (published clinical or pre-clinical trials) surrounding ethno- and
>>> > alternative- medicinal treatments, for semi-automated meta analysis
>>> > and treatment guidance. In order for this to work, a lot of
>>> > technical challenges have to be overcome:
>>>
>>> > My initial tally from PubMed puts the number of articles at over
>>> > 70,000; based on visual inspection, many of these are not actually
>>> > applicable, but there are limited filtering options via the Entrez
>>> > web API. Machine learning techniques would probably be very helpful
>>> > at scoring articles for applicability, and ignoring ones that are
>>> > clearly inapplicable.
>>>
>>> > In order to perform meta-analysis and treatment guidance, the
>>> > article needs to be mined for treatment, condition, circumstances of
>>> > treatment and condition, and whether it was successful or not (with
>>> > some p value and sample size). Most of this is not available as
>>> > standard metadata for the studies, and must be mined from the text.
>>>
>>> > In addition, not all studies are equal. Methodological errors, lack
>>> > of reproduciblity, and so forth can all render a study meaningless.
>>> > Thus, studies must have a scoring mechanism so you can avoid
>>> > tainting meta-analyses with biased data. This scoring mechanism will
>>> > probably include the impact factor of the journal, the g/h-index of
>>> > the authors, the number of (non self) citations, etc.
>>>
>>> > As you can see, each of these is meaty, and all of them need to be
>>> > taken care of to get good results :) If anyone is interested in
>>> > getting some serious natural language processing/data mining/machine
>>> > learning practice, I'd love to involve you. There's no reason I
>>> > should have all the fun!
>>>
>>> http://mail.python.org/pipermail/trizpug/2012-August/001920.html
>>> > I'm still in the planning stages for most of the stuff; I have the
>>> > pubmed extraction code pretty well nailed, and I have a solid
>>> > outline for the article disqualification (create a feature vector
>>> > out of topic and abstract bigrams, MeSH subject headings and
>>> > journal, use a SVM discriminator and manually generate a RoC curve
>>> > to determine the cutoff score) but I'm still very up in the air
>>> > regarding NL extraction of things like sample size, significance,
>>> > etc. If you'd like to learn more I would of course be happy to go
>>> > over my thoughts on the matter and we can play around with some
>>> > code.
>>>
>>> _______________________________________________
>>> open-science mailing list
>>> open-science at lists.okfn.org
>>> http://lists.okfn.org/mailman/listinfo/open-science
>>>
>>>
>>> _______________________________________________
>>> open-science-dev mailing list
>>> open-science-dev at lists.okfn.org
>>> http://lists.okfn.org/mailman/listinfo/open-science-dev
>>>
>>>
>>
>>
>> --
>>
>> ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
>> http://github.com/teleyinex
>> http://www.flickr.com/photos/teleyinex
>>
>> ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
>> Por favor, NO utilice formatos de archivo propietarios para el
>> intercambio de documentos, como DOC y XLS, sino PDF, HTML, RTF, TXT, CSV
>> o cualquier otro que no obligue a utilizar un programa de un
>> fabricante concreto para tratar la informaci?n contenida en ?l.
>>
>> ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
>>
>
>
> _______________________________________________
> open-access mailing list
> open-access at lists.okfn.org
> http://lists.okfn.org/mailman/listinfo/open-access
>
>


-- 
Peter Murray-Rust
Reader in Molecular Informatics
Unilever Centre, Dep. Of Chemistry
University of Cambridge
CB2 1EW, UK
+44-1223-763069
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120818/ff1dfea8/attachment.htm>

From tfmorris at gmail.com  Sat Aug 18 20:53:06 2012
From: tfmorris at gmail.com (Tom Morris)
Date: Sat, 18 Aug 2012 15:53:06 -0400
Subject: [open-bibliography] PyBossa crowdsourcing & other new OKFN stuff
Message-ID: <CAE9vqEHs8oieY8fGNt3sdYdHb5dPwWGAJh-Th6wpt5oYpYUVgQ@mail.gmail.com>

So apparently the PyBossa which was mentioned in passing on the other
thread is a free, hosted crowd sourcing solution which is under
development by the OKFN.  It's hosted at pybossa.com and is so named
because it's a reimplementation in Python of the Bossa component of
the Berkeley's BOINC project
http://boinc.berkeley.edu/trac/wiki/BossaIntro

There's a bunch of bibliographic data tasks which could be crowd
sourced, such as reviewing the validity of proposed author merges or
other significant changes, but I'm also interested in the more general
question about how one discovers new OKFN projects.

Is there a central announcement mailing list somewhere?  It seems I'm
constantly stumbling across new projects.  Sometimes it's just an
existing project that's taken a new name (e.g. DataHub), but often its
something truly new that I've just never heard of.  It'd be great if
there was a low overhead way of keeping up.  Recommendations?

Tom


From pm286 at cam.ac.uk  Sat Aug 18 22:38:36 2012
From: pm286 at cam.ac.uk (Peter Murray-Rust)
Date: Sat, 18 Aug 2012 22:38:36 +0100
Subject: [open-bibliography] [Open-access] [open-science-dev] Fwd:
 [open-science] fw: Python NLTK/data mining/machine learning project of
 public research data, anyone interested?
In-Reply-To: <CAOFbRmJN6CqzOWFPpN7dy6kpb-ooVjmUB-2xFT36nOHPOyNg_g@mail.gmail.com>
References: <mailman.17.1345197601.15227.trizpug@python.org>
	<87has1n7sc.fsf@pobox.com>
	<CABPM+woF6+ytXYCtYp07P2feFgwutSpkTgK-59+ckoEeYHhYmw@mail.gmail.com>
	<CAGS_vfpKyoRTpAtWXamgJhAPtRt-RD=3ZYdroC4jy8b17cTc9w@mail.gmail.com>
	<CABPM+wr+Rs1V2e6aBtC=-Wo74aVpmtG-RvPH49CKYe5ooQ27+g@mail.gmail.com>
	<CAD2k14MZsPquK2c=+0SDqubhjBA+wQPOTr2q1j+z-FB56hAQ0A@mail.gmail.com>
	<CAOFbRmJN6CqzOWFPpN7dy6kpb-ooVjmUB-2xFT36nOHPOyNg_g@mail.gmail.com>
Message-ID: <CAD2k14NTnfmDD7BdnFe_qtfkvFOq=4nydEQKXPo_qBZfvnSEnw@mail.gmail.com>

Thanks, Nathan.
No one is going to put pressure on you to do anything - but the
oppoertunity for jointly coalescing projects is there.

On Sat, Aug 18, 2012 at 10:14 PM, Nathan Rice <
nathan.alexander.rice at gmail.com> wrote:

>
> Wow, I'm surprised this has made its way around as much as it has.  I
> suppose if a project makes a jaded bioinformatics guy like me excited,
> it shouldn't surprise me that others would find it interesting too.
>
> There is huge potential in automation which is why it's exciting. I
realise I didn't answer your original question - see later.

I'm still a little bit intimidated by the amount of work that will be
> involved in getting a really solid, fully automated pipeline.


Then take things at a pace that can be managed. No-one has to be a hero by
themselves.


> I'm
> trying to take it a step at a time.  I've almost finished a curated
> list of plants, experimental molecules and compounds, and I'm fine
> tuning the pubmed search code to reduce the initial signal to noise
> ratio.
>

It may be that when you expose those you will find overlap with other
people.

>
> I'm still not sure exactly how I want to go about selecting articles
> to use as the training data set for article filtration.  A manually
> curated list would probably work best, but given the number of
> features that are available, I expect that the training set would need
> to be at least 1,000 articles in size to get decent results.  This
> might just be one of those cases where I need to bite the bullet, put
> a large pot of coffee on, and get to work.
>
> To do content mining properly requires a considerable annotated corpus.
Generally it's split 3 ways - training, testing and validation. But such a
corpus is very valuable. Unfortunately copyright normally means it can't be
redistributed (I've had this fight with publishers). However that will
change as they realise that alienating the world won't work as they aren't
very competent totalitarians.


> >   - PDF hacking (I have done a lot of this but we need more. Open font
> info.
> > Postscript reconstruction
>
> I have played with this a bit, one issue that is frustrating is many
> PDFanalysis tools will randomly insert spaces due to font kerning, and
> will order text based on vertical position on the page, rather than
> preserving column order.  If there is a PDF text extraction tool that
> doesn't do these I would love to know.
>

I work with PDFBox and pull this out character by character. I throw away
all sequential information and only use coordinates and font-size.  This
works pretty well for me. I can see some excessive kernings and ligatures
may defeat it but at present I suspect I get less than 1 spurious space per
1000 chars. And remember we also have vocabularies to help tune this.

>
>
> >   - shallow natural language processing and NPL resources (e.g. vocabs,
> > character sets)
> >   - classification techniques (e.g. Lucene/Solr) for text and diagrams
> >
> > I think if we harness all these we will have a large step change in the
> > automation of extraction of scientific information from "the literature".
> >
> > And one-by-one the publishers will come to us because they will need us.
>
> It is really a shame that metadata isn't more standardized for journal
> articles.


We have been addressing this in the Open biblio project(s). BibJSON acts as
an unofficial normalization of article metadata. If you mean
domain-specific metadata then we have to do this ourselves - and I am
confident we can - it will be better than keywords (I have little faith in
them)


>  Pubmed MeSH terms and chemical lists are OK but there is so
> much more that could be annotated for the article.
>
> I am very interested in generic classifiers at this level.



> > Timescale - about 1 year to have something major to report - about 5
> years
> > to change the way scientific information is managed.
>
> Scientific articles seem like the perfect place for semantic metadata.
>  In particular, clinical trial articles should have a nice, standard
> set of metadata artifacts for computer analysis, since they are so
> cookie cutter.
>

I looked at this 2-3 years ago - for clinical trials on nutrition. IIRC the
abstracts were very useful metadata - they were structured and used
standard-ish terms. I think they could be NLP'ed quite well.

>
>
> I have actually already invested some of my (unfortunately scant)
> resources into having people go through mined pubmed articles and
> create metadata annotations.  Unfortunately, without a lot of machine
> learning input set filtration, this is going to cost at least
> $10,000-20,000 USD to finish for my purposes, and more every time the
> list is updated.  It would be much better to get really solid
> algorithms together so that nobody has to incur costs on this
> magnitude :)
>

Ultimately humans have to validate the metadata. You need an
inter-annotator agreement. In chemistry we found that the maximum agreement
between expert human chemists was 93% for whether a phrase was a chemical
or not. Machines by definition cannot do better than this.

It's tempting to develop crowdsourcing for annotation but it's important
that the crowd is part of the project, not just passive slaves.

>
>
> --
Peter Murray-Rust
Reader in Molecular Informatics
Unilever Centre, Dep. Of Chemistry
University of Cambridge
CB2 1EW, UK
+44-1223-763069
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120818/4118803e/attachment-0001.htm>

From nathan.alexander.rice at gmail.com  Sat Aug 18 22:14:03 2012
From: nathan.alexander.rice at gmail.com (Nathan Rice)
Date: Sat, 18 Aug 2012 17:14:03 -0400
Subject: [open-bibliography] [Open-access] [open-science-dev] Fwd:
 [open-science] fw: Python NLTK/data mining/machine learning project of
 public research data, anyone interested?
In-Reply-To: <CAD2k14MZsPquK2c=+0SDqubhjBA+wQPOTr2q1j+z-FB56hAQ0A@mail.gmail.com>
References: <mailman.17.1345197601.15227.trizpug@python.org>
	<87has1n7sc.fsf@pobox.com>
	<CABPM+woF6+ytXYCtYp07P2feFgwutSpkTgK-59+ckoEeYHhYmw@mail.gmail.com>
	<CAGS_vfpKyoRTpAtWXamgJhAPtRt-RD=3ZYdroC4jy8b17cTc9w@mail.gmail.com>
	<CABPM+wr+Rs1V2e6aBtC=-Wo74aVpmtG-RvPH49CKYe5ooQ27+g@mail.gmail.com>
	<CAD2k14MZsPquK2c=+0SDqubhjBA+wQPOTr2q1j+z-FB56hAQ0A@mail.gmail.com>
Message-ID: <CAOFbRmJN6CqzOWFPpN7dy6kpb-ooVjmUB-2xFT36nOHPOyNg_g@mail.gmail.com>

>> Thanks Daniel!
>>
>> The original email was from Nathan Rice who I've cc'd into the thread, if
>> you could reply to him.
>>
>> Nathan - Tom forwarded your email to the Open Knowledge Foundation lists,
>> we have some people interested in text and data mining on board so hopefully
>> they'll be able to offer some advice/assistance! We run periodic hack days,
>> in the UK mostly, but one coming up in Helsinki on 18 September if you'd be
>> interested to do a remote demo, at the last one there was some work on
>> content mining phylogenetic trees
>> http://rossmounce.co.uk/2012/07/17/content-mining-for-phylogenetic-data/
>>

Wow, I'm surprised this has made its way around as much as it has.  I
suppose if a project makes a jaded bioinformatics guy like me excited,
it shouldn't surprise me that others would find it interesting too.

> This is really exciting. Be aware, Nathan, that if you come up with a good
> idea (which you have) you are likely to end up making it work!

I'm still a little bit intimidated by the amount of work that will be
involved in getting a really solid, fully automated pipeline.  I'm
trying to take it a step at a time.  I've almost finished a curated
list of plants, experimental molecules and compounds, and I'm fine
tuning the pubmed search code to reduce the initial signal to noise
ratio.

I'm still not sure exactly how I want to go about selecting articles
to use as the training data set for article filtration.  A manually
curated list would probably work best, but given the number of
features that are available, I expect that the training set would need
to be at least 1,000 articles in size to get decent results.  This
might just be one of those cases where I need to bite the bullet, put
a large pot of coffee on, and get to work.

> I am copying the three lists because this is still very general but we
> should condense it later to more specialist topics.
>
> There is a real need for coordinating content-mining. There are a  number of
> threads:
> * tools to systematically retrieve "documents", such as PubCrawler whcih
> crawls publishers' sites. At some stage we shall press the button and
> collect a large amount of metadata. This distributes very nicely with one
> scraper per publisher (about 100 in science according to Ross Mounce). We
> have scrapers for some of the major ones but will need this for others.
> First one required is BMC (which is ultra-legal as it's CC-BY)
> * protocols for content-mining - legal and technical
> * collation of experiences and groups in content-mining. This is really
> important. I've been hacking in this area for 3 months and I am not
> connected with other efforts nor they with me (apart from about 3 bio-
> ones). We need to bring content-miners together. Since a major barrier is
> publisher FUD let's give each other confidence. Also the OKF's Datahub is a
> great place to put Open results.
> * collation of technologies. There are at least:
>   - scrapers
>   - PDF hacking (I have done a lot of this but we need more. Open font info.
> Postscript reconstruction

I have played with this a bit, one issue that is frustrating is many
PDFanalysis tools will randomly insert spaces due to font kerning, and
will order text based on vertical position on the page, rather than
preserving column order.  If there is a PDF text extraction tool that
doesn't do these I would love to know.

>   - bit-map hacking for diagram analysis (I am excited by the quality of
> modern scientific diagrams and think there can be a lot of automation)

Although this is slightly outside the scope of what I'm doing, I do
agree this is very interesting.

>   - shallow natural language processing and NPL resources (e.g. vocabs,
> character sets)
>   - classification techniques (e.g. Lucene/Solr) for text and diagrams
>
> I think if we harness all these we will have a large step change in the
> automation of extraction of scientific information from "the literature".
>
> And one-by-one the publishers will come to us because they will need us.

It is really a shame that metadata isn't more standardized for journal
articles.  Pubmed MeSH terms and chemical lists are OK but there is so
much more that could be annotated for the article.

> Timescale - about 1 year to have something major to report - about 5 years
> to change the way scientific information is managed.

Scientific articles seem like the perfect place for semantic metadata.
 In particular, clinical trial articles should have a nice, standard
set of metadata artifacts for computer analysis, since they are so
cookie cutter.

>> Jenny
>>
>> On Fri, Aug 17, 2012 at 1:39 PM, Daniel Lombra?a Gonz?lez
>> <teleyinex at gmail.com> wrote:
>>>
>>> Hi,
>>>
>>> I think this project could be interesting for PyBossa in the sense that
>>> some data-mining and validation could be done by humans :-) I can give Tom
>>> more details if PyBossa is helpful :-)

I have actually already invested some of my (unfortunately scant)
resources into having people go through mined pubmed articles and
create metadata annotations.  Unfortunately, without a lot of machine
learning input set filtration, this is going to cost at least
$10,000-20,000 USD to finish for my purposes, and more every time the
list is updated.  It would be much better to get really solid
algorithms together so that nobody has to incur costs on this
magnitude :)


>>> Cheers,
>>>
>>> Daniel
>>>
>>> On Fri, Aug 17, 2012 at 2:09 PM, Jenny Molloy <jcmcoppice12 at gmail.com>
>>> wrote:
>>>>
>>>> Hi All
>>>>
>>>> Apologies for cross-posting but this came out on open-science and I
>>>> thought it might be of interest to some of you as well
>>>>
>>>> Jenny
>>>>
>>>> ---------- Forwarded message ----------
>>>> From: Tom Roche <Tom_Roche at pobox.com>
>>>> Date: Fri, Aug 17, 2012 at 11:15 AM
>>>> Subject: [open-science] fw: Python NLTK/data mining/machine learning
>>>> project of public research data, anyone interested?
>>>> To: open-science at lists.okfn.org
>>>>
>>>>
>>>>
>>>> Dunno if the following is OT for this group, but thought this thread
>>>> from the local PUG might be of interest. (Note I don't know the
>>>> author personally; reply to him, not me.)
>>>>
>>>> http://mail.python.org/pipermail/trizpug/2012-August/001919.html
>>>> > Nathan Rice nathan.alexander.rice at gmail.com
>>>> > Thu Aug 16 20:31:00 CEST 2012
>>>>
>>>> > Hi All,
>>>>
>>>> > Normally, my projects are pretty boring, and I prefer to endure the
>>>> > suffering in solitary silence. As luck would have it though, I
>>>> > actually have an interesting project on my plate currently, and I
>>>> > think it is cool enough that I wanted to give other people the
>>>> > opportunity to stick their noses in and provide input or play with
>>>> > some code.
>>>>
>>>> > I am currently involved in compiling a database of medical data
>>>> > (published clinical or pre-clinical trials) surrounding ethno- and
>>>> > alternative- medicinal treatments, for semi-automated meta analysis
>>>> > and treatment guidance. In order for this to work, a lot of
>>>> > technical challenges have to be overcome:
>>>>
>>>> > My initial tally from PubMed puts the number of articles at over
>>>> > 70,000; based on visual inspection, many of these are not actually
>>>> > applicable, but there are limited filtering options via the Entrez
>>>> > web API. Machine learning techniques would probably be very helpful
>>>> > at scoring articles for applicability, and ignoring ones that are
>>>> > clearly inapplicable.
>>>>
>>>> > In order to perform meta-analysis and treatment guidance, the
>>>> > article needs to be mined for treatment, condition, circumstances of
>>>> > treatment and condition, and whether it was successful or not (with
>>>> > some p value and sample size). Most of this is not available as
>>>> > standard metadata for the studies, and must be mined from the text.
>>>>
>>>> > In addition, not all studies are equal. Methodological errors, lack
>>>> > of reproduciblity, and so forth can all render a study meaningless.
>>>> > Thus, studies must have a scoring mechanism so you can avoid
>>>> > tainting meta-analyses with biased data. This scoring mechanism will
>>>> > probably include the impact factor of the journal, the g/h-index of
>>>> > the authors, the number of (non self) citations, etc.
>>>>
>>>> > As you can see, each of these is meaty, and all of them need to be
>>>> > taken care of to get good results :) If anyone is interested in
>>>> > getting some serious natural language processing/data mining/machine
>>>> > learning practice, I'd love to involve you. There's no reason I
>>>> > should have all the fun!
>>>>
>>>> http://mail.python.org/pipermail/trizpug/2012-August/001920.html
>>>> > I'm still in the planning stages for most of the stuff; I have the
>>>> > pubmed extraction code pretty well nailed, and I have a solid
>>>> > outline for the article disqualification (create a feature vector
>>>> > out of topic and abstract bigrams, MeSH subject headings and
>>>> > journal, use a SVM discriminator and manually generate a RoC curve
>>>> > to determine the cutoff score) but I'm still very up in the air
>>>> > regarding NL extraction of things like sample size, significance,
>>>> > etc. If you'd like to learn more I would of course be happy to go
>>>> > over my thoughts on the matter and we can play around with some
>>>> > code.
>>>>
>>>> _______________________________________________
>>>> open-science mailing list
>>>> open-science at lists.okfn.org
>>>> http://lists.okfn.org/mailman/listinfo/open-science
>>>>
>>>>
>>>> _______________________________________________
>>>> open-science-dev mailing list
>>>> open-science-dev at lists.okfn.org
>>>> http://lists.okfn.org/mailman/listinfo/open-science-dev
>>>>
>>>
>>>
>>>
>>> --
>>>
>>> ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
>>> http://github.com/teleyinex
>>> http://www.flickr.com/photos/teleyinex
>>>
>>> ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
>>> Por favor, NO utilice formatos de archivo propietarios para el
>>> intercambio de documentos, como DOC y XLS, sino PDF, HTML, RTF, TXT, CSV
>>> o cualquier otro que no obligue a utilizar un programa de un
>>> fabricante concreto para tratar la informaci?n contenida en ?l.
>>>
>>> ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
>>
>>

Thanks again all,

Nathan
>>
>> _______________________________________________
>> open-access mailing list
>> open-access at lists.okfn.org
>> http://lists.okfn.org/mailman/listinfo/open-access
>>
>
>
>
> --
> Peter Murray-Rust
> Reader in Molecular Informatics
> Unilever Centre, Dep. Of Chemistry
> University of Cambridge
> CB2 1EW, UK
> +44-1223-763069


From teleyinex at gmail.com  Mon Aug 20 07:17:19 2012
From: teleyinex at gmail.com (=?UTF-8?Q?Daniel_Lombra=C3=B1a_Gonz=C3=A1lez?=)
Date: Mon, 20 Aug 2012 08:17:19 +0200
Subject: [open-bibliography] [Open-access] [open-science-dev] Fwd:
 [open-science] fw: Python NLTK/data mining/machine learning project of
 public research data, anyone interested?
In-Reply-To: <CAD2k14NTnfmDD7BdnFe_qtfkvFOq=4nydEQKXPo_qBZfvnSEnw@mail.gmail.com>
References: <mailman.17.1345197601.15227.trizpug@python.org>
	<87has1n7sc.fsf@pobox.com>
	<CABPM+woF6+ytXYCtYp07P2feFgwutSpkTgK-59+ckoEeYHhYmw@mail.gmail.com>
	<CAGS_vfpKyoRTpAtWXamgJhAPtRt-RD=3ZYdroC4jy8b17cTc9w@mail.gmail.com>
	<CABPM+wr+Rs1V2e6aBtC=-Wo74aVpmtG-RvPH49CKYe5ooQ27+g@mail.gmail.com>
	<CAD2k14MZsPquK2c=+0SDqubhjBA+wQPOTr2q1j+z-FB56hAQ0A@mail.gmail.com>
	<CAOFbRmJN6CqzOWFPpN7dy6kpb-ooVjmUB-2xFT36nOHPOyNg_g@mail.gmail.com>
	<CAD2k14NTnfmDD7BdnFe_qtfkvFOq=4nydEQKXPo_qBZfvnSEnw@mail.gmail.com>
Message-ID: <CAGS_vfp711peppC==h2006EChpA_BNTRxeQmdXoNLPr2nxt1EQ@mail.gmail.com>

Hi,

I think that Peter has raised several good points about human validation
and how they should not be treated like "monkeys" working for the project
:-)


From pm286 at cam.ac.uk  Mon Aug 20 12:50:01 2012
From: pm286 at cam.ac.uk (Peter Murray-Rust)
Date: Mon, 20 Aug 2012 12:50:01 +0100
Subject: [open-bibliography] [Open-access] [open-science-dev] Fwd:
 [open-science] fw: Python NLTK/data mining/machine learning project of
 public research data, anyone interested?
In-Reply-To: <551F3248-65D9-4280-8F2E-F9C5490E3FFE@inria.fr>
References: <mailman.17.1345197601.15227.trizpug@python.org>
	<87has1n7sc.fsf@pobox.com>
	<CABPM+woF6+ytXYCtYp07P2feFgwutSpkTgK-59+ckoEeYHhYmw@mail.gmail.com>
	<CAGS_vfpKyoRTpAtWXamgJhAPtRt-RD=3ZYdroC4jy8b17cTc9w@mail.gmail.com>
	<CABPM+wr+Rs1V2e6aBtC=-Wo74aVpmtG-RvPH49CKYe5ooQ27+g@mail.gmail.com>
	<CAD2k14MZsPquK2c=+0SDqubhjBA+wQPOTr2q1j+z-FB56hAQ0A@mail.gmail.com>
	<CAOFbRmJN6CqzOWFPpN7dy6kpb-ooVjmUB-2xFT36nOHPOyNg_g@mail.gmail.com>
	<CAD2k14NTnfmDD7BdnFe_qtfkvFOq=4nydEQKXPo_qBZfvnSEnw@mail.gmail.com>
	<551F3248-65D9-4280-8F2E-F9C5490E3FFE@inria.fr>
Message-ID: <CAD2k14PO2=JxdF38OJsFh4e+-noxHF2Amiy=ukfLnD=Cd9EXwQ@mail.gmail.com>

On Mon, Aug 20, 2012 at 8:21 AM, Laurent Romary <laurent.romary at inria.fr>wrote:

> Dear all,
> Catching up this thread to answer on two issues:
> 1. Metadata formats for scholarly papers: this is something we have been
> extensively been working on in the context of the EU PEER project (
> http://www.peerproject.eu/) where we mapped heterogeneous metadata coming
> from quite a wide spectrum of commercial STM publishers onto a single TEI
> (Text Encoding Initiative) based format.  The format is documented under
> http://hal.inria.fr/hal-00659856 and we did a short communication (with
> slides) about it at the 2010 TEI conference:
> http://hal.inria.fr/inria-00537302 Using the TEI (an open standards) as a
> reference brings the work on scholarly document into a wider corpus of
> encoded texts (see http://www.tei-c.org/Activities/Projects/ just to get
> a partial overview of the TEI community)
>

Sounds good - especially since we can use the PEER project as an effective
authority.  Mark, etc. - how does this map onto BibJSON? TEI used to be
SGML and very complex - I hope this is a subset and can be used as XML
without the need for an SGML parser.

2. We (Patrice Lopez, CCed) developed an open source library of metadata,
> but also full text extraction tools which creates a TEI based
> representation of a pdf given as input. This could be particularly
> interesting for your work I think, and an opportunity to share forces
> there.  See https://sourceforge.net/projects/grobid/
>

Java - great for me! I'll checkout and look later but this seems very
valuable. This is exactly what we need to interface with PubCrawler (unless
you also have a crawler). We can then  download metadata from all the
publishers - we already have scrapers for several. That gives us the
metadata framework for then managing Open bibliography and using it.




Cheers,
> Laurent
>
> Le 18 ao?t 2012 ? 23:38, Peter Murray-Rust a ?crit :
>
> Thanks, Nathan.
> No one is going to put pressure on you to do anything - but the
> oppoertunity for jointly coalescing projects is there.
>
> On Sat, Aug 18, 2012 at 10:14 PM, Nathan Rice <
> nathan.alexander.rice at gmail.com> wrote:
>
>>
>> Wow, I'm surprised this has made its way around as much as it has.  I
>> suppose if a project makes a jaded bioinformatics guy like me excited,
>> it shouldn't surprise me that others would find it interesting too.
>>
>> There is huge potential in automation which is why it's exciting. I
> realise I didn't answer your original question - see later.
>
> I'm still a little bit intimidated by the amount of work that will be
>> involved in getting a really solid, fully automated pipeline.
>
>
> Then take things at a pace that can be managed. No-one has to be a hero by
> themselves.
>
>
>> I'm
>> trying to take it a step at a time.  I've almost finished a curated
>> list of plants, experimental molecules and compounds, and I'm fine
>> tuning the pubmed search code to reduce the initial signal to noise
>> ratio.
>>
>
> It may be that when you expose those you will find overlap with other
> people.
>
>>
>> I'm still not sure exactly how I want to go about selecting articles
>> to use as the training data set for article filtration.  A manually
>> curated list would probably work best, but given the number of
>> features that are available, I expect that the training set would need
>> to be at least 1,000 articles in size to get decent results.  This
>> might just be one of those cases where I need to bite the bullet, put
>> a large pot of coffee on, and get to work.
>>
>> To do content mining properly requires a considerable annotated corpus.
> Generally it's split 3 ways - training, testing and validation. But such a
> corpus is very valuable. Unfortunately copyright normally means it can't be
> redistributed (I've had this fight with publishers). However that will
> change as they realise that alienating the world won't work as they aren't
> very competent totalitarians.
>
>
>> >   - PDF hacking (I have done a lot of this but we need more. Open font
>> info.
>> > Postscript reconstruction
>>
>> I have played with this a bit, one issue that is frustrating is many
>> PDFanalysis tools will randomly insert spaces due to font kerning, and
>> will order text based on vertical position on the page, rather than
>> preserving column order.  If there is a PDF text extraction tool that
>> doesn't do these I would love to know.
>>
>
> I work with PDFBox and pull this out character by character. I throw away
> all sequential information and only use coordinates and font-size.  This
> works pretty well for me. I can see some excessive kernings and ligatures
> may defeat it but at present I suspect I get less than 1 spurious space per
> 1000 chars. And remember we also have vocabularies to help tune this.
>
>>
>>
>> >   - shallow natural language processing and NPL resources (e.g. vocabs,
>> > character sets)
>> >   - classification techniques (e.g. Lucene/Solr) for text and diagrams
>> >
>> > I think if we harness all these we will have a large step change in the
>> > automation of extraction of scientific information from "the
>> literature".
>> >
>> > And one-by-one the publishers will come to us because they will need us.
>>
>> It is really a shame that metadata isn't more standardized for journal
>> articles.
>
>
> We have been addressing this in the Open biblio project(s). BibJSON acts
> as an unofficial normalization of article metadata. If you mean
> domain-specific metadata then we have to do this ourselves - and I am
> confident we can - it will be better than keywords (I have little faith in
> them)
>
>
>>  Pubmed MeSH terms and chemical lists are OK but there is so
>> much more that could be annotated for the article.
>>
>> I am very interested in generic classifiers at this level.
>
>
>
>> > Timescale - about 1 year to have something major to report - about 5
>> years
>> > to change the way scientific information is managed.
>>
>> Scientific articles seem like the perfect place for semantic metadata.
>>  In particular, clinical trial articles should have a nice, standard
>> set of metadata artifacts for computer analysis, since they are so
>> cookie cutter.
>>
>
> I looked at this 2-3 years ago - for clinical trials on nutrition. IIRC
> the abstracts were very useful metadata - they were structured and used
> standard-ish terms. I think they could be NLP'ed quite well.
>
>>
>>
>> I have actually already invested some of my (unfortunately scant)
>> resources into having people go through mined pubmed articles and
>> create metadata annotations.  Unfortunately, without a lot of machine
>> learning input set filtration, this is going to cost at least
>> $10,000-20,000 USD to finish for my purposes, and more every time the
>> list is updated.  It would be much better to get really solid
>> algorithms together so that nobody has to incur costs on this
>> magnitude :)
>>
>
> Ultimately humans have to validate the metadata. You need an
> inter-annotator agreement. In chemistry we found that the maximum agreement
> between expert human chemists was 93% for whether a phrase was a chemical
> or not. Machines by definition cannot do better than this.
>
> It's tempting to develop crowdsourcing for annotation but it's important
> that the crowd is part of the project, not just passive slaves.
>
>>
>>
>> --
> Peter Murray-Rust
> Reader in Molecular Informatics
> Unilever Centre, Dep. Of Chemistry
> University of Cambridge
> CB2 1EW, UK
> +44-1223-763069
> _______________________________________________
> open-access mailing list
> open-access at lists.okfn.org
> http://lists.okfn.org/mailman/listinfo/open-access
>
>
> Laurent Romary
> INRIA & HUB-IDSL
> laurent.romary at inria.fr
>
>
>
>


-- 
Peter Murray-Rust
Reader in Molecular Informatics
Unilever Centre, Dep. Of Chemistry
University of Cambridge
CB2 1EW, UK
+44-1223-763069
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120820/e39ddb75/attachment.htm>

From pm286 at cam.ac.uk  Mon Aug 20 13:55:51 2012
From: pm286 at cam.ac.uk (Peter Murray-Rust)
Date: Mon, 20 Aug 2012 13:55:51 +0100
Subject: [open-bibliography] [Open-access] [open-science-dev] Fwd:
 [open-science] fw: Python NLTK/data mining/machine learning project of
 public research data, anyone interested?
In-Reply-To: <508C6D9D-D3FE-4647-AF4F-42EC500A6F8D@inria.fr>
References: <mailman.17.1345197601.15227.trizpug@python.org>
	<87has1n7sc.fsf@pobox.com>
	<CABPM+woF6+ytXYCtYp07P2feFgwutSpkTgK-59+ckoEeYHhYmw@mail.gmail.com>
	<CAGS_vfpKyoRTpAtWXamgJhAPtRt-RD=3ZYdroC4jy8b17cTc9w@mail.gmail.com>
	<CABPM+wr+Rs1V2e6aBtC=-Wo74aVpmtG-RvPH49CKYe5ooQ27+g@mail.gmail.com>
	<CAD2k14MZsPquK2c=+0SDqubhjBA+wQPOTr2q1j+z-FB56hAQ0A@mail.gmail.com>
	<CAOFbRmJN6CqzOWFPpN7dy6kpb-ooVjmUB-2xFT36nOHPOyNg_g@mail.gmail.com>
	<CAD2k14NTnfmDD7BdnFe_qtfkvFOq=4nydEQKXPo_qBZfvnSEnw@mail.gmail.com>
	<551F3248-65D9-4280-8F2E-F9C5490E3FFE@inria.fr>
	<CAD2k14PO2=JxdF38OJsFh4e+-noxHF2Amiy=ukfLnD=Cd9EXwQ@mail.gmail.com>
	<508C6D9D-D3FE-4647-AF4F-42EC500A6F8D@inria.fr>
Message-ID: <CAD2k14NDNY38W-3kUxcsAphJx95XqaWGrUN3SQ3h8iF6hB5oQA@mail.gmail.com>

On Mon, Aug 20, 2012 at 1:17 PM, Laurent Romary <laurent.romary at inria.fr>wrote:

>
> Le 20 ao?t 2012 ? 13:50, Peter Murray-Rust a ?crit :
>
> That was 20 years ago ;-). It now comes with a very flexible customization
> platform allowing you to dumb down things to your needs. [disclosure: I
> have chaired the TEI technical council 2008-2011]. Remember also that the
> TEI people have been part of the setting up of XML (e.g. Michael
> Sperberg-McQueen, co-editor of both the TEI and the XML rec. in late 90's).
> And it comes automatically with the Oxygen XML editor.
>
>
Indeed. I ran the XML-DEV list in 1997 so knew many people involved in TEI.

>
> 2. We (Patrice Lopez, CCed) developed an open source library of metadata,
>> but also full text extraction tools which creates a TEI based
>> representation of a pdf given as input. This could be particularly
>> interesting for your work I think, and an opportunity to share forces
>> there.  See https://sourceforge.net/projects/grobid/
>>
>
> Java - great for me! I'll checkout and look later but this seems very
> valuable. This is exactly what we need to interface with PubCrawler (unless
> you also have a crawler). We can then  download metadata from all the
> publishers - we already have scrapers for several. That gives us the
> metadata framework for then managing Open bibliography and using it.
>
>
> It depends what you mean by crawler. Can you say more about this?
>

Recursively crawls publisher->journal->issue->article


>
> In PEER, I was the one to develop the XSLT stylesheets from the various
> publishers' formats (ScholarOne, various versions of NLM, Elsevier, Nature,
> ...) to TEI. I have never managed to put this together in SF, but could zip
> this to however would want to push things further.
>
> This assumes that one has XML. I am working on the assumption that we have
the PDF only (and that's an advantage for getting the material out of
diagrams)

P.


-- 
Peter Murray-Rust
Reader in Molecular Informatics
Unilever Centre, Dep. Of Chemistry
University of Cambridge
CB2 1EW, UK
+44-1223-763069
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120820/66c43eae/attachment-0001.htm>

From pm286 at cam.ac.uk  Mon Aug 20 14:19:05 2012
From: pm286 at cam.ac.uk (Peter Murray-Rust)
Date: Mon, 20 Aug 2012 14:19:05 +0100
Subject: [open-bibliography] [Open-access] [open-science-dev] Fwd:
 [open-science] fw: Python NLTK/data mining/machine learning project of
 public research data, anyone interested?
In-Reply-To: <EB8B7825-2F28-413A-B570-4AD61853E64D@inria.fr>
References: <mailman.17.1345197601.15227.trizpug@python.org>
	<87has1n7sc.fsf@pobox.com>
	<CABPM+woF6+ytXYCtYp07P2feFgwutSpkTgK-59+ckoEeYHhYmw@mail.gmail.com>
	<CAGS_vfpKyoRTpAtWXamgJhAPtRt-RD=3ZYdroC4jy8b17cTc9w@mail.gmail.com>
	<CABPM+wr+Rs1V2e6aBtC=-Wo74aVpmtG-RvPH49CKYe5ooQ27+g@mail.gmail.com>
	<CAD2k14MZsPquK2c=+0SDqubhjBA+wQPOTr2q1j+z-FB56hAQ0A@mail.gmail.com>
	<CAOFbRmJN6CqzOWFPpN7dy6kpb-ooVjmUB-2xFT36nOHPOyNg_g@mail.gmail.com>
	<CAD2k14NTnfmDD7BdnFe_qtfkvFOq=4nydEQKXPo_qBZfvnSEnw@mail.gmail.com>
	<551F3248-65D9-4280-8F2E-F9C5490E3FFE@inria.fr>
	<CAD2k14PO2=JxdF38OJsFh4e+-noxHF2Amiy=ukfLnD=Cd9EXwQ@mail.gmail.com>
	<508C6D9D-D3FE-4647-AF4F-42EC500A6F8D@inria.fr>
	<CAD2k14NDNY38W-3kUxcsAphJx95XqaWGrUN3SQ3h8iF6hB5oQA@mail.gmail.com>
	<EB8B7825-2F28-413A-B570-4AD61853E64D@inria.fr>
Message-ID: <CAD2k14OEGbg=COQiU+Dt2Bz6qn5aDVsgkbB7-B-r8SLb5tFqpQ@mail.gmail.com>

On Mon, Aug 20, 2012 at 2:04 PM, Laurent Romary <laurent.romary at inria.fr>wrote:

>
>
> It depends what you mean by crawler. Can you say more about this?
>
>
> Recursively crawls publisher->journal->issue->article
>
>
> This we do not have.
>
>
Great!! then we interface directly


>
>
>>
>> In PEER, I was the one to develop the XSLT stylesheets from the various
>> publishers' formats (ScholarOne, various versions of NLM, Elsevier, Nature,
>> ...) to TEI. I have never managed to put this together in SF, but could zip
>> this to however would want to push things further.
>>
>> This assumes that one has XML. I am working on the assumption that we
> have the PDF only (and that's an advantage for getting the material out of
> diagrams)
>
>
> We actually worked on both scenario in PEER. So the software on SF work
> directly with PDFs and the stylesheets are there because we also got a huge
> amount of data directly from publishers.
>

Problem with material from publishers is that it is usually a one-off
provision of material and there are often legal constraints

P.


> Laurent
>
>
> P.
>
>
> --
> Peter Murray-Rust
> Reader in Molecular Informatics
> Unilever Centre, Dep. Of Chemistry
> University of Cambridge
> CB2 1EW, UK
> +44-1223-763069
>
>
> Laurent Romary
> INRIA & HUB-IDSL
> laurent.romary at inria.fr
>
>
>
>


-- 
Peter Murray-Rust
Reader in Molecular Informatics
Unilever Centre, Dep. Of Chemistry
University of Cambridge
CB2 1EW, UK
+44-1223-763069
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120820/8a58c75c/attachment.htm>

From sam.leon at okfn.org  Mon Aug 20 15:20:28 2012
From: sam.leon at okfn.org (Sam Leon)
Date: Mon, 20 Aug 2012 15:20:28 +0100
Subject: [open-bibliography]  PyBossa crowdsourcing & other new OKFN stuff
Message-ID: <CAOn=SAa6gy9VJD13D8vp9j+OELCBfEufLUY9CzWfdjANzQAeeA@mail.gmail.com>

Hi Tom,

I've just seen your message to the Open Biblio list below.

Great to hear that you can see applications for PyBossa for bibliographic
data. If you have any further specific ideas be sure to copy in the Open
Bibliography Developers'
List<http://lists.okfn.org/mailman/listinfo/openbiblio-dev> as
there are already a number of people working in the field who have ideas
along the lines you suggest. (I'm happy to intro you personally if that
would be helpful).

Re: OKFN projects and updates about them, your feedback is really helpful.
We're currently re-designing our website and the end result will make it
much easier to find and track OKFN projects.

In the meantime, please make sure that you...

(1) Sign up to OKFN
Discuss<http://lists.okfn.org/mailman/listinfo/okfn-discuss/>.
This is where all our projects are announced.
(2) Sign up to OKFN Labs
<http://lists.okfn.org/mailman/listinfo/okfn-labs>and have visited the
Labs
site <http://okfnlabs.org/>. PyBossa updates and those from other smaller
OKFN projects are all sent there. The Labs list is currently brimming with
activity and exciting ideas, so it's an exiting place to be!

Do let me know if there is any more information you need in the meantime.

All the best,
Sam

---------- Forwarded message ----------
From: Tom Morris <tfmorris at gmail.com>
Date: Sat, Aug 18, 2012 at 9:53 PM
Subject: [open-bibliography] PyBossa crowdsourcing & other new OKFN stuff
To: List for Working Group on Open Bibliographic Data
<open-bibliography at lists.okfn.org>


So apparently the PyBossa which was mentioned in passing on the other
thread is a free, hosted crowd sourcing solution which is under
development by the OKFN.  It's hosted at pybossa.com and is so named
because it's a reimplementation in Python of the Bossa component of
the Berkeley's BOINC project
http://boinc.berkeley.edu/trac/wiki/BossaIntro

There's a bunch of bibliographic data tasks which could be crowd
sourced, such as reviewing the validity of proposed author merges or
other significant changes, but I'm also interested in the more general
question about how one discovers new OKFN projects.

Is there a central announcement mailing list somewhere?  It seems I'm
constantly stumbling across new projects.  Sometimes it's just an
existing project that's taken a new name (e.g. DataHub), but often its
something truly new that I've just never heard of.  It'd be great if
there was a low overhead way of keeping up.  Recommendations?

Tom

_______________________________________________
open-bibliography mailing list
open-bibliography at lists.okfn.org
http://lists.okfn.org/mailman/listinfo/open-bibliography

-- 
Sam Leon
Community Coordinator
Open Knowledge Foundation
http://okfn.org/
Twitter: @noeL_maS
Skype: samedleon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120820/5d9fa517/attachment.htm>

From laurent.romary at inria.fr  Mon Aug 20 13:17:22 2012
From: laurent.romary at inria.fr (Laurent Romary)
Date: Mon, 20 Aug 2012 14:17:22 +0200
Subject: [open-bibliography] [Open-access] [open-science-dev] Fwd:
	[open-science] fw: Python NLTK/data mining/machine learning
	project of public research data, anyone interested?
In-Reply-To: <CAD2k14PO2=JxdF38OJsFh4e+-noxHF2Amiy=ukfLnD=Cd9EXwQ@mail.gmail.com>
References: <mailman.17.1345197601.15227.trizpug@python.org>
	<87has1n7sc.fsf@pobox.com>
	<CABPM+woF6+ytXYCtYp07P2feFgwutSpkTgK-59+ckoEeYHhYmw@mail.gmail.com>
	<CAGS_vfpKyoRTpAtWXamgJhAPtRt-RD=3ZYdroC4jy8b17cTc9w@mail.gmail.com>
	<CABPM+wr+Rs1V2e6aBtC=-Wo74aVpmtG-RvPH49CKYe5ooQ27+g@mail.gmail.com>
	<CAD2k14MZsPquK2c=+0SDqubhjBA+wQPOTr2q1j+z-FB56hAQ0A@mail.gmail.com>
	<CAOFbRmJN6CqzOWFPpN7dy6kpb-ooVjmUB-2xFT36nOHPOyNg_g@mail.gmail.com>
	<CAD2k14NTnfmDD7BdnFe_qtfkvFOq=4nydEQKXPo_qBZfvnSEnw@mail.gmail.com>
	<551F3248-65D9-4280-8F2E-F9C5490E3FFE@inria.fr>
	<CAD2k14PO2=JxdF38OJsFh4e+-noxHF2Amiy=ukfLnD=Cd9EXwQ@mail.gmail.com>
Message-ID: <508C6D9D-D3FE-4647-AF4F-42EC500A6F8D@inria.fr>


Le 20 ao?t 2012 ? 13:50, Peter Murray-Rust a ?crit :

> 
> 
> On Mon, Aug 20, 2012 at 8:21 AM, Laurent Romary <laurent.romary at inria.fr> wrote:
> Dear all,
> Catching up this thread to answer on two issues:
> 1. Metadata formats for scholarly papers: this is something we have been extensively been working on in the context of the EU PEER project (http://www.peerproject.eu/) where we mapped heterogeneous metadata coming from quite a wide spectrum of commercial STM publishers onto a single TEI (Text Encoding Initiative) based format.  The format is documented under http://hal.inria.fr/hal-00659856 and we did a short communication (with slides) about it at the 2010 TEI conference:  http://hal.inria.fr/inria-00537302 Using the TEI (an open standards) as a reference brings the work on scholarly document into a wider corpus of encoded texts (see http://www.tei-c.org/Activities/Projects/ just to get a partial overview of the TEI community)
> 
> Sounds good - especially since we can use the PEER project as an effective authority.  Mark, etc. - how does this map onto BibJSON? TEI used to be SGML and very complex - I hope this is a subset and can be used as XML without the need for an SGML parser.

That was 20 years ago ;-). It now comes with a very flexible customization platform allowing you to dumb down things to your needs. [disclosure: I have chaired the TEI technical council 2008-2011]. Remember also that the TEI people have been part of the setting up of XML (e.g. Michael Sperberg-McQueen, co-editor of both the TEI and the XML rec. in late 90's). And it comes automatically with the Oxygen XML editor.

> 
> 2. We (Patrice Lopez, CCed) developed an open source library of metadata, but also full text extraction tools which creates a TEI based representation of a pdf given as input. This could be particularly interesting for your work I think, and an opportunity to share forces there.  See https://sourceforge.net/projects/grobid/
> 
> Java - great for me! I'll checkout and look later but this seems very valuable. This is exactly what we need to interface with PubCrawler (unless you also have a crawler). We can then  download metadata from all the publishers - we already have scrapers for several. That gives us the metadata framework for then managing Open bibliography and using it.

It depends what you mean by crawler. Can you say more about this?

In PEER, I was the one to develop the XSLT stylesheets from the various publishers' formats (ScholarOne, various versions of NLM, Elsevier, Nature, ...) to TEI. I have never managed to put this together in SF, but could zip this to however would want to push things further.

> 
> 
> 
> 
> Cheers,
> Laurent
> 
> Le 18 ao?t 2012 ? 23:38, Peter Murray-Rust a ?crit :
> 
>> Thanks, Nathan.
>> No one is going to put pressure on you to do anything - but the oppoertunity for jointly coalescing projects is there.
>> 
>> On Sat, Aug 18, 2012 at 10:14 PM, Nathan Rice <nathan.alexander.rice at gmail.com> wrote:
>> 
>> Wow, I'm surprised this has made its way around as much as it has.  I
>> suppose if a project makes a jaded bioinformatics guy like me excited,
>> it shouldn't surprise me that others would find it interesting too.
>> 
>> There is huge potential in automation which is why it's exciting. I realise I didn't answer your original question - see later. 
>> 
>> I'm still a little bit intimidated by the amount of work that will be
>> involved in getting a really solid, fully automated pipeline.  
>> 
>> Then take things at a pace that can be managed. No-one has to be a hero by themselves.
>>  
>> I'm
>> trying to take it a step at a time.  I've almost finished a curated
>> list of plants, experimental molecules and compounds, and I'm fine
>> tuning the pubmed search code to reduce the initial signal to noise
>> ratio.
>> 
>> It may be that when you expose those you will find overlap with other people. 
>> 
>> I'm still not sure exactly how I want to go about selecting articles
>> to use as the training data set for article filtration.  A manually
>> curated list would probably work best, but given the number of
>> features that are available, I expect that the training set would need
>> to be at least 1,000 articles in size to get decent results.  This
>> might just be one of those cases where I need to bite the bullet, put
>> a large pot of coffee on, and get to work.
>> 
>> To do content mining properly requires a considerable annotated corpus. Generally it's split 3 ways - training, testing and validation. But such a corpus is very valuable. Unfortunately copyright normally means it can't be redistributed (I've had this fight with publishers). However that will change as they realise that alienating the world won't work as they aren't very competent totalitarians.
>>  
>> >   - PDF hacking (I have done a lot of this but we need more. Open font info.
>> > Postscript reconstruction
>> 
>> I have played with this a bit, one issue that is frustrating is many
>> PDFanalysis tools will randomly insert spaces due to font kerning, and
>> will order text based on vertical position on the page, rather than
>> preserving column order.  If there is a PDF text extraction tool that
>> doesn't do these I would love to know.
>> 
>> I work with PDFBox and pull this out character by character. I throw away all sequential information and only use coordinates and font-size.  This works pretty well for me. I can see some excessive kernings and ligatures may defeat it but at present I suspect I get less than 1 spurious space per 1000 chars. And remember we also have vocabularies to help tune this.
>> 
>> 
>> >   - shallow natural language processing and NPL resources (e.g. vocabs,
>> > character sets)
>> >   - classification techniques (e.g. Lucene/Solr) for text and diagrams
>> >
>> > I think if we harness all these we will have a large step change in the
>> > automation of extraction of scientific information from "the literature".
>> >
>> > And one-by-one the publishers will come to us because they will need us.
>> 
>> It is really a shame that metadata isn't more standardized for journal
>> articles.
>> 
>> We have been addressing this in the Open biblio project(s). BibJSON acts as an unofficial normalization of article metadata. If you mean domain-specific metadata then we have to do this ourselves - and I am confident we can - it will be better than keywords (I have little faith in them) 
>>  
>>  Pubmed MeSH terms and chemical lists are OK but there is so
>> much more that could be annotated for the article.
>> 
>> I am very interested in generic classifiers at this level.
>> 
>>  
>> > Timescale - about 1 year to have something major to report - about 5 years
>> > to change the way scientific information is managed.
>> 
>> Scientific articles seem like the perfect place for semantic metadata.
>>  In particular, clinical trial articles should have a nice, standard
>> set of metadata artifacts for computer analysis, since they are so
>> cookie cutter.
>> 
>> I looked at this 2-3 years ago - for clinical trials on nutrition. IIRC the abstracts were very useful metadata - they were structured and used standard-ish terms. I think they could be NLP'ed quite well.
>> 
>> 
>> I have actually already invested some of my (unfortunately scant)
>> resources into having people go through mined pubmed articles and
>> create metadata annotations.  Unfortunately, without a lot of machine
>> learning input set filtration, this is going to cost at least
>> $10,000-20,000 USD to finish for my purposes, and more every time the
>> list is updated.  It would be much better to get really solid
>> algorithms together so that nobody has to incur costs on this
>> magnitude :)
>> 
>> Ultimately humans have to validate the metadata. You need an inter-annotator agreement. In chemistry we found that the maximum agreement between expert human chemists was 93% for whether a phrase was a chemical or not. Machines by definition cannot do better than this. 
>> 
>> It's tempting to develop crowdsourcing for annotation but it's important that the crowd is part of the project, not just passive slaves.
>> 
>> 
>> -- 
>> Peter Murray-Rust
>> Reader in Molecular Informatics
>> Unilever Centre, Dep. Of Chemistry
>> University of Cambridge
>> CB2 1EW, UK
>> +44-1223-763069
>> _______________________________________________
>> open-access mailing list
>> open-access at lists.okfn.org
>> http://lists.okfn.org/mailman/listinfo/open-access
> 
> Laurent Romary
> INRIA & HUB-IDSL
> laurent.romary at inria.fr
> 
> 
> 
> 
> 
> 
> -- 
> Peter Murray-Rust
> Reader in Molecular Informatics
> Unilever Centre, Dep. Of Chemistry
> University of Cambridge
> CB2 1EW, UK
> +44-1223-763069

Laurent Romary
INRIA & HUB-IDSL
laurent.romary at inria.fr



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120820/8d8d1da3/attachment-0001.htm>

From laurent.romary at inria.fr  Mon Aug 20 14:04:01 2012
From: laurent.romary at inria.fr (Laurent Romary)
Date: Mon, 20 Aug 2012 15:04:01 +0200
Subject: [open-bibliography] [Open-access] [open-science-dev] Fwd:
	[open-science] fw: Python NLTK/data mining/machine learning
	project of public research data, anyone interested?
In-Reply-To: <CAD2k14NDNY38W-3kUxcsAphJx95XqaWGrUN3SQ3h8iF6hB5oQA@mail.gmail.com>
References: <mailman.17.1345197601.15227.trizpug@python.org>
	<87has1n7sc.fsf@pobox.com>
	<CABPM+woF6+ytXYCtYp07P2feFgwutSpkTgK-59+ckoEeYHhYmw@mail.gmail.com>
	<CAGS_vfpKyoRTpAtWXamgJhAPtRt-RD=3ZYdroC4jy8b17cTc9w@mail.gmail.com>
	<CABPM+wr+Rs1V2e6aBtC=-Wo74aVpmtG-RvPH49CKYe5ooQ27+g@mail.gmail.com>
	<CAD2k14MZsPquK2c=+0SDqubhjBA+wQPOTr2q1j+z-FB56hAQ0A@mail.gmail.com>
	<CAOFbRmJN6CqzOWFPpN7dy6kpb-ooVjmUB-2xFT36nOHPOyNg_g@mail.gmail.com>
	<CAD2k14NTnfmDD7BdnFe_qtfkvFOq=4nydEQKXPo_qBZfvnSEnw@mail.gmail.com>
	<551F3248-65D9-4280-8F2E-F9C5490E3FFE@inria.fr>
	<CAD2k14PO2=JxdF38OJsFh4e+-noxHF2Amiy=ukfLnD=Cd9EXwQ@mail.gmail.com>
	<508C6D9D-D3FE-4647-AF4F-42EC500A6F8D@inria.fr>
	<CAD2k14NDNY38W-3kUxcsAphJx95XqaWGrUN3SQ3h8iF6hB5oQA@mail.gmail.com>
Message-ID: <EB8B7825-2F28-413A-B570-4AD61853E64D@inria.fr>




Le 20 ao?t 2012 ? 14:55, Peter Murray-Rust a ?crit :
>> 
>> 2. We (Patrice Lopez, CCed) developed an open source library of metadata, but also full text extraction tools which creates a TEI based representation of a pdf given as input. This could be particularly interesting for your work I think, and an opportunity to share forces there.  See https://sourceforge.net/projects/grobid/
>> 
>> Java - great for me! I'll checkout and look later but this seems very valuable. This is exactly what we need to interface with PubCrawler (unless you also have a crawler). We can then  download metadata from all the publishers - we already have scrapers for several. That gives us the metadata framework for then managing Open bibliography and using it.
> 
> It depends what you mean by crawler. Can you say more about this?
> 
> Recursively crawls publisher->journal->issue->article

This we do not have.

>  
> 
> In PEER, I was the one to develop the XSLT stylesheets from the various publishers' formats (ScholarOne, various versions of NLM, Elsevier, Nature, ...) to TEI. I have never managed to put this together in SF, but could zip this to however would want to push things further.
> 
> This assumes that one has XML. I am working on the assumption that we have the PDF only (and that's an advantage for getting the material out of diagrams)

We actually worked on both scenario in PEER. So the software on SF work directly with PDFs and the stylesheets are there because we also got a huge amount of data directly from publishers.
Laurent

> 
> P.
>  
> 
> -- 
> Peter Murray-Rust
> Reader in Molecular Informatics
> Unilever Centre, Dep. Of Chemistry
> University of Cambridge
> CB2 1EW, UK
> +44-1223-763069

Laurent Romary
INRIA & HUB-IDSL
laurent.romary at inria.fr



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120820/ad0a15b8/attachment.htm>

From thomas.johnson at oregonstate.edu  Tue Aug 21 00:33:36 2012
From: thomas.johnson at oregonstate.edu (Tom Johnson)
Date: Mon, 20 Aug 2012 16:33:36 -0700
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
 Piece of WorldCat to Play With
In-Reply-To: <502F1710.5070503@kcoyle.net>
References: <CAD47Kz7kiBbweXRQ0-zE5u8roPKTG9fG-3FvRZmZRohcq1DEbg@mail.gmail.com>
	<CANQ_vu18wmGY+8iUoocTiZajt7ZH0m+3dFU+uQXYZ6+-pzDegg@mail.gmail.com>
	<502E75D1.1040803@kcoyle.net>
	<CAJcYwHaGXKjVQyCfDjp0FDHjdcRX9hWACkxDBZXgS6D6eMd2bA@mail.gmail.com>
	<CAJeHiNHJw-9JiX+=CU5YWSMunNJaBWB8htB6FhuBMvvSfwy5Fg@mail.gmail.com>
	<502E8911.6000307@kcoyle.net>
	<CAE9vqEHqbPryCNB0yb-XBL3ki+zbGcvLmdYLmofPjfs-tegTqA@mail.gmail.com>
	<CAJeHiNEfC_W=KQK74o=oqz6-ffz+qhNgD5cRvWnY8A-1g+SnxQ@mail.gmail.com>
	<502F1710.5070503@kcoyle.net>
Message-ID: <CAJeHiNHB6Yn+LOXqDpV9Y9P9j+G-AXONHm=Dg3m7xtW65MZwRA@mail.gmail.com>

A SPARQL endpoint is up at http://worldcat.library.oregonstate.edu/sparql

Anyone with a SPARQL client is welcome to use that endpoint for as long as
it exists. Performance looks just fine to me for the moment, but please let
me know if you run into any problems.

4store provides a test query page, letting you write queries in the
browser. It is at http://worldcat.library.oregonstate.edu/test

Try something like:

   DESCRIBE <http://www.worldcat.org/isbn/0879692243>

or:

   DESCRIBE <http://id.worldcat.org/fast/898705>

The djubby front-end I put up should let you hit items by oclcnum or isbn,
like so:

   http://data.library.oregonstate.edu/worldcat/oclc/14588496
   http://data.library.oregonstate.edu/worldcat/isbn/9780879692247

It doesn't display blank nodes very intelligently, though, so I'm not sure
how useful it will be in practice. It would be better to have something
which will at least seek out labels for object URIs.

I'm also thinking about loading in the VIAF and FAST graphs, since that
should help with the kind of visualization Karen is talking about. In the
meanwhile, there is an endpoint. I'll make an effort to keep it up live and
up to date until I say otherwise, so please use it as you see fit.

As an afterthought: anyone have any advice about meeting the attribution
terms of the ODC license?

- Tom


On Fri, Aug 17, 2012 at 9:16 PM, Karen Coyle <kcoyle at kcoyle.net> wrote:

> Tom,
>
> I'll poke around and see if anyone is using any "easy" visualization
> software. The Nat'l Lib. of Spain did some neat stuff with Graphviz, but I
> have no idea what that took. [1]
>
> For basic functionality, I'd love to see a minimal web form that would
> launch a search via SPARQL. (Pubby may do this -- I don't see a screen shot
> that answers this for me.) It would probably have to be limited to
> searching on only certain values, but that's ok for a start. At minimum,
> pulling up everything with the subject URI of an OCLC number or an object
> with the VIAF URI. Since the URI patterns for those are set, it should be
> possible to have a form for just the number and to fill in the full URI for
> the search. An even easier alternative would be to supply the SPARQL
> patterns for those searches, to be launched from within a web page. I could
> find examples of what I mean if this isn't clear. In any case, being able
> to do some minimal searching seems to be a best first step.
>
> Thanks,
>
> kc
> [1] http://bne.linkeddata.es/**graphvis/<http://bne.linkeddata.es/graphvis/>
>
>
> On 8/17/12 4:14 PM, Tom Johnson wrote:
>
>> I'm not having any trouble loading it (except that it is slow). I'm
>> fussing with the best way to configure 4store to handle ~80 million
>> triples.
>>
>> The data looks good to me.
>>
>> I'm also putting up a pubby-like front end.
>>
>> I'm not sure what the real cost of running a SPARQL endpoint for a
>> dataset like this is going to look like, or whether I can support it in
>> the long run. Still, I'm interested in hearing what people would want to
>> see and how they would use it if I (or Oregon State) were to run
>> services on it.
>>
>> - Tom
>>
>> On Fri, Aug 17, 2012 at 3:50 PM, Tom Morris <tfmorris at gmail.com
>> <mailto:tfmorris at gmail.com>> wrote:
>>
>>     Karen,
>>
>>     On Fri, Aug 17, 2012 at 2:10 PM, Karen Coyle <kcoyle at kcoyle.net
>>     <mailto:kcoyle at kcoyle.net>> wrote:
>>      > Luc, I think this reflects an answer to your question. As with
>>     much that
>>      > happens in computer technology, some of us have to depend on
>>     others. I find
>>      > making our wishes clear helps guide those kind souls who have the
>>     necessary
>>      > skills. Maybe we can work further with Tom and others to spell
>>     out what we
>>      > need for this to be usable for us.
>>
>>     Does having this data loaded into a triple store help you?  What types
>>     of things would it enable?
>>
>>     It seems like it might be marginally better than a raw RDF file, but
>>     it seems like it would still take a fair amount of work to do anything
>>     useful with it.
>>
>>     Tom
>>
>>     p.s. I'm curious to see if the other Tom is able to load it using his
>>     tools because it looks to me like it contains invalid URIs (embedded
>>     spaces) which may cause RDF tools to choke depending on how picky they
>>     are about parsing.
>>
>>      >
>>      > What we still need in the RDF world is the application that would
>>     do for the
>>      > Semantic Web what Mosaic did for the Web: make it viewable and
>>     usable by the
>>      > non-programmer. But first we have to have an actual Semantic Web,
>>     and I
>>      > think that's still in progress in a strict sense.
>>      >
>>      > kc
>>      >
>>      >
>>      > On 8/17/12 10:31 AM, Tom Johnson wrote:
>>      >>
>>      >> I'm in the process of putting up a triplestore w/ endpoint
>>     already. I
>>      >> have no problem sending out a link.
>>      >>
>>      >> I'm in an all day meeting today, so it might not happen until
>>     the weekend.
>>      >>
>>      >> On Fri, Aug 17, 2012 at 9:53 AM, Luc Gauvreau <lgovro at gmail.com
>>     <mailto:lgovro at gmail.com>
>>      >> <mailto:lgovro at gmail.com <mailto:lgovro at gmail.com>>> wrote:
>>      >>
>>      >>     Bonjour,
>>      >>
>>      >>     A very good question!
>>      >>
>>      >>     Multiple projects about linked datas and RDF, but who has the
>>      >>     expertise to use it?
>>      >>
>>      >>     Only experts and "geeks"?
>>      >>
>>      >>     Is it possible for an "amateur" to use these kind of format,
>>     files
>>      >>     and codes?
>>      >>
>>      >>     A kind of "Linked data and RDF for dummies" will be very
>>     usefull.
>>      >>     Merci,
>>      >>
>>      >>     Luc Gauvreau
>>      >>     (Montr?al, Qu?bec)
>>      >>
>>      >>
>>      >>
>>      >>     2012/8/17 Karen Coyle <kcoyle at kcoyle.net
>>     <mailto:kcoyle at kcoyle.net> <mailto:kcoyle at kcoyle.net
>>
>>     <mailto:kcoyle at kcoyle.net>>>
>>      >>
>>      >>
>>      >>         I would love it if someone could put this in a triple
>>     store for
>>      >>         others to play with. How difficult is that?
>>      >>
>>      >>         kc
>>      >>
>>      >>
>>      >>         On 8/17/12 8:58 AM, Jonathan Gray wrote:
>>      >>
>>      >>             ---------- Forwarded message ----------
>>      >>             From: Richard Wallis <richard.wallis at dataliberate._**
>> _com
>>      >>             <mailto:richard.wallis@**dataliberate.com<richard.wallis at dataliberate.com>
>>     <mailto:richard.wallis@**dataliberate.com<richard.wallis at dataliberate.com>
>> >>>
>>      >>             Date: Fri, Aug 17, 2012 at 5:42 PM
>>      >>             Subject: [LODLAM] Get Yourself a Linked Data Piece of
>>      >>             WorldCat to Play With
>>      >>             To: lod-lam at googlegroups.com
>>     <mailto:lod-lam at googlegroups.**com <lod-lam at googlegroups.com>>
>> <mailto:lod-lam at googlegroups.**com <lod-lam at googlegroups.com>
>>     <mailto:lod-lam at googlegroups.**com <lod-lam at googlegroups.com>>>
>>      >>
>>      >>
>>      >>             In case you missed the press release earlier this
>> week.
>>      >>
>>      >>             You can now download a significant number of RDF
>> triples
>>      >>             describing
>>      >>             the most highly held 1.2 million resources in
>> WorldCat.
>>      >>               Licensed
>>      >>             under ODC-BY.
>>      >>
>>      >>             I've posted more details on my blog:
>>      >>
>>      >>
>>     http://dataliberate.com/2012/_**_08/get-yourself-a-linked-**
>> data-__piece-of-worldcat-to-**play-__with/<http://dataliberate.com/2012/__08/get-yourself-a-linked-data-__piece-of-worldcat-to-play-__with/>
>>      >>
>>      >>
>>      >>
>>     <http://dataliberate.com/2012/**08/get-yourself-a-linked-data-**
>> piece-of-worldcat-to-play-**with/<http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-worldcat-to-play-with/>
>> >
>>      >>
>>      >>             ~Richard
>>      >>
>>
>>
>>
> --
> Karen Coyle
> kcoyle at kcoyle.net http://kcoyle.net
> ph: 1-510-540-7596
> m: 1-510-435-8234
> skype: kcoylenet
>
> ______________________________**_________________
> open-bibliography mailing list
> open-bibliography at lists.okfn.**org <open-bibliography at lists.okfn.org>
> http://lists.okfn.org/mailman/**listinfo/open-bibliography<http://lists.okfn.org/mailman/listinfo/open-bibliography>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120820/8c2b7422/attachment-0001.htm>

From richard.wallis at oclc.org  Tue Aug 21 08:42:15 2012
From: richard.wallis at oclc.org (Richard Wallis)
Date: Tue, 21 Aug 2012 08:42:15 +0100
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
 Piece of WorldCat to Play With
In-Reply-To: <CAJeHiNHB6Yn+LOXqDpV9Y9P9j+G-AXONHm=Dg3m7xtW65MZwRA@mail.gmail.com>
Message-ID: <CC58FA67.1D4A%richard.wallis@oclc.org>

Hi Tom,

It is great to see that you have done this ? just the kind of thing we were
hoping folks would do when we released this download.

You say performance looks fine, I would be interested what hardware you are
running it on.  I have dropped 4Store on my MacBook Air and I am pleasantly
surprised with the performance.

Regarding your attribution question, you may find our attribution guidelines
useful: http://www.oclc.org/data/attribution.html

~Richard



On 21/08/2012 00:33, "Tom Johnson" <thomas.johnson at oregonstate.edu> wrote:

> A SPARQL endpoint is up at http://worldcat.library.oregonstate.edu/sparql?
> 
> Anyone with a SPARQL client is welcome to use that endpoint for as long as it
> exists. Performance looks just fine to me for the moment, but please let me
> know if you run into any problems.
> 
> 4store provides a test query page, letting you write queries in the browser.
> It is at http://worldcat.library.oregonstate.edu/test
> 
> Try something like:
> 
> ? ?DESCRIBE <http://www.worldcat.org/isbn/0879692243>
> 
> or:?
> 
> ? ?DESCRIBE <http://id.worldcat.org/fast/898705>
> 
> The djubby front-end I put up should let you hit items by oclcnum or isbn,
> like so:
> 
> ? ?http://data.library.oregonstate.edu/worldcat/oclc/14588496
> ? ?http://data.library.oregonstate.edu/worldcat/isbn/9780879692247
> 
> It doesn't display blank nodes very intelligently, though, so I'm not sure how
> useful it will be in practice. It would be better to have something which will
> at least seek out labels for object URIs.?
> 
> I'm also thinking about loading in the VIAF and FAST graphs, since that should
> help with the kind of visualization Karen is talking about. In the meanwhile,
> there is an endpoint. I'll make an effort to keep it up live and up to date
> until I say otherwise, so please use it as you see fit.
> 
> As an afterthought: anyone have any advice about meeting the attribution terms
> of the ODC license?
> 
> - Tom
> 
> 
> On Fri, Aug 17, 2012 at 9:16 PM, Karen Coyle <kcoyle at kcoyle.net> wrote:
>> Tom,
>> 
>> I'll poke around and see if anyone is using any "easy" visualization
>> software. The Nat'l Lib. of Spain did some neat stuff with Graphviz, but I
>> have no idea what that took. [1]
>> 
>> For basic functionality, I'd love to see a minimal web form that would launch
>> a search via SPARQL. (Pubby may do this -- I don't see a screen shot that
>> answers this for me.) It would probably have to be limited to searching on
>> only certain values, but that's ok for a start. At minimum, pulling up
>> everything with the subject URI of an OCLC number or an object with the VIAF
>> URI. Since the URI patterns for those are set, it should be possible to have
>> a form for just the number and to fill in the full URI for the search. An
>> even easier alternative would be to supply the SPARQL patterns for those
>> searches, to be launched from within a web page. I could find examples of
>> what I mean if this isn't clear. In any case, being able to do some minimal
>> searching seems to be a best first step.
>> 
>> Thanks,
>> 
>> kc
>> [1] http://bne.linkeddata.es/graphvis/ <http://bne.linkeddata.es/graphvis/>
>> 
>> 
>> On 8/17/12 4:14 PM, Tom Johnson wrote:
>>> I'm not having any trouble loading it (except that it is slow). I'm
>>> fussing with the best way to configure 4store to handle ~80 million triples.
>>> 
>>> The data looks good to me.
>>> 
>>> I'm also putting up a pubby-like front end.
>>> 
>>> I'm not sure what the real cost of running a SPARQL endpoint for a
>>> dataset like this is going to look like, or whether I can support it in
>>> the long run. Still, I'm interested in hearing what people would want to
>>> see and how they would use it if I (or Oregon State) were to run
>>> services on it.
>>> 
>>> - Tom
>>> 
>>> On Fri, Aug 17, 2012 at 3:50 PM, Tom Morris <tfmorris at gmail.com
>>> <mailto:tfmorris at gmail.com>> wrote:
>>> 
>>> ? ? Karen,
>>> 
>>> ? ? On Fri, Aug 17, 2012 at 2:10 PM, Karen Coyle <kcoyle at kcoyle.net
>>> ? ? <mailto:kcoyle at kcoyle.net>> wrote:
>>> ? ? ?> Luc, I think this reflects an answer to your question. As with
>>> ? ? much that
>>> ? ? ?> happens in computer technology, some of us have to depend on
>>> ? ? others. I find
>>> ? ? ?> making our wishes clear helps guide those kind souls who have the
>>> ? ? necessary
>>> ? ? ?> skills. Maybe we can work further with Tom and others to spell
>>> ? ? out what we
>>> ? ? ?> need for this to be usable for us.
>>> 
>>> ? ? Does having this data loaded into a triple store help you? ?What types
>>> ? ? of things would it enable?
>>> 
>>> ? ? It seems like it might be marginally better than a raw RDF file, but
>>> ? ? it seems like it would still take a fair amount of work to do anything
>>> ? ? useful with it.
>>> 
>>> ? ? Tom
>>> 
>>> ? ? p.s. I'm curious to see if the other Tom is able to load it using his
>>> ? ? tools because it looks to me like it contains invalid URIs (embedded
>>> ? ? spaces) which may cause RDF tools to choke depending on how picky they
>>> ? ? are about parsing.
>>> 
>>> ? ? ?>
>>> ? ? ?> What we still need in the RDF world is the application that would
>>> ? ? do for the
>>> ? ? ?> Semantic Web what Mosaic did for the Web: make it viewable and
>>> ? ? usable by the
>>> ? ? ?> non-programmer. But first we have to have an actual Semantic Web,
>>> ? ? and I
>>> ? ? ?> think that's still in progress in a strict sense.
>>> ? ? ?>
>>> ? ? ?> kc
>>> ? ? ?>
>>> ? ? ?>
>>> ? ? ?> On 8/17/12 10:31 AM, Tom Johnson wrote:
>>> ? ? ?>>
>>> ? ? ?>> I'm in the process of putting up a triplestore w/ endpoint
>>> ? ? already. I
>>> ? ? ?>> have no problem sending out a link.
>>> ? ? ?>>
>>> ? ? ?>> I'm in an all day meeting today, so it might not happen until
>>> ? ? the weekend.
>>> ? ? ?>>
>>> ? ? ?>> On Fri, Aug 17, 2012 at 9:53 AM, Luc Gauvreau <lgovro at gmail.com
>>> ? ? <mailto:lgovro at gmail.com>
>>> ? ? ?>> <mailto:lgovro at gmail.com <mailto:lgovro at gmail.com>>> wrote:
>>> ? ? ?>>
>>> ? ? ?>> ? ? Bonjour,
>>> ? ? ?>>
>>> ? ? ?>> ? ? A very good question!
>>> ? ? ?>>
>>> ? ? ?>> ? ? Multiple projects about linked datas and RDF, but who has the
>>> ? ? ?>> ? ? expertise to use it?
>>> ? ? ?>>
>>> ? ? ?>> ? ? Only experts and "geeks"?
>>> ? ? ?>>
>>> ? ? ?>> ? ? Is it possible for an "amateur" to use these kind of format,
>>> ? ? files
>>> ? ? ?>> ? ? and codes?
>>> ? ? ?>>
>>> ? ? ?>> ? ? A kind of "Linked data and RDF for dummies" will be very
>>> ? ? usefull.
>>> ? ? ?>> ? ? Merci,
>>> ? ? ?>>
>>> ? ? ?>> ? ? Luc Gauvreau
>>> ? ? ?>> ? ? (Montr?al, Qu?bec)
>>> ? ? ?>>
>>> ? ? ?>>
>>> ? ? ?>>
>>> ? ? ?>> ? ? 2012/8/17 Karen Coyle <kcoyle at kcoyle.net
>>> ? ? <mailto:kcoyle at kcoyle.net> <mailto:kcoyle at kcoyle.net
>>> 
>>> ? ? <mailto:kcoyle at kcoyle.net>>>
>>> ? ? ?>>
>>> ? ? ?>>
>>> ? ? ?>> ? ? ? ? I would love it if someone could put this in a triple
>>> ? ? store for
>>> ? ? ?>> ? ? ? ? others to play with. How difficult is that?
>>> ? ? ?>>
>>> ? ? ?>> ? ? ? ? kc
>>> ? ? ?>>
>>> ? ? ?>>
>>> ? ? ?>> ? ? ? ? On 8/17/12 8:58 AM, Jonathan Gray wrote:
>>> ? ? ?>>
>>> ? ? ?>> ? ? ? ? ? ? ---------- Forwarded message ----------
>>> ? ? ?>> ? ? ? ? ? ? From: Richard Wallis <richard.wallis at dataliberate.__com
>>> ? ? ?>> ? ? ? ? ? ? <mailto:richard.wallis at dataliberate.com
>>> <mailto:richard.wallis at dataliberate.com>
>>> ? ? <mailto:richard.wallis at dataliberate.com
>>> <mailto:richard.wallis at dataliberate.com> >>>
>>> ? ? ?>> ? ? ? ? ? ? Date: Fri, Aug 17, 2012 at 5:42 PM
>>> ? ? ?>> ? ? ? ? ? ? Subject: [LODLAM] Get Yourself a Linked Data Piece of
>>> ? ? ?>> ? ? ? ? ? ? WorldCat to Play With
>>> ? ? ?>> ? ? ? ? ? ? To: lod-lam at googlegroups.com
>>> ? ? <mailto:lod-lam at googlegroups.com <mailto:lod-lam at googlegroups.com> >
>>> <mailto:lod-lam at googlegroups.com <mailto:lod-lam at googlegroups.com>
>>> ? ? <mailto:lod-lam at googlegroups.com <mailto:lod-lam at googlegroups.com> >>
>>> ? ? ?>>
>>> ? ? ?>>
>>> ? ? ?>> ? ? ? ? ? ? In case you missed the press release earlier this week.
>>> ? ? ?>>
>>> ? ? ?>> ? ? ? ? ? ? You can now download a significant number of RDF triples
>>> ? ? ?>> ? ? ? ? ? ? describing
>>> ? ? ?>> ? ? ? ? ? ? the most highly held 1.2 million resources in WorldCat.
>>> ? ? ?>> ? ? ? ? ? ? ? Licensed
>>> ? ? ?>> ? ? ? ? ? ? under ODC-BY.
>>> ? ? ?>>
>>> ? ? ?>> ? ? ? ? ? ? I've posted more details on my blog:
>>> ? ? ?>>
>>> ? ? ?>>
>>> ? ? 
>>> http://dataliberate.com/2012/__08/get-yourself-a-linked-data-__piece-of-worl
>>> dcat-to-play-__with/
>>> <http://dataliberate.com/2012/__08/get-yourself-a-linked-data-__piece-of-wor
>>> ldcat-to-play-__with/>
>>> ? ? ?>>
>>> ? ? ?>>
>>> ? ? ?>>
>>> ? ? 
>>> <http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-worldca
>>> t-to-play-with/
>>> <http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-worldca
>>> t-to-play-with/> >
>>> ? ? ?>>
>>> ? ? ?>> ? ? ? ? ? ? ~Richard
>>> ? ? ?>>
>>> 
>>> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120821/53c0af92/attachment.htm>

From thomas.johnson at oregonstate.edu  Wed Aug 22 03:28:51 2012
From: thomas.johnson at oregonstate.edu (Tom Johnson)
Date: Tue, 21 Aug 2012 19:28:51 -0700
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
 Piece of WorldCat to Play With
In-Reply-To: <CC58FA67.1D4A%richard.wallis@oclc.org>
References: <CAJeHiNHB6Yn+LOXqDpV9Y9P9j+G-AXONHm=Dg3m7xtW65MZwRA@mail.gmail.com>
	<CC58FA67.1D4A%richard.wallis@oclc.org>
Message-ID: <CAJeHiNEsq3_Z-Z_Zo2kK3zRtKJJMh9aeS=uaoRvvuqVSSn+BKQ@mail.gmail.com>

> It is great to see that you have done this ? just the kind of thing we
were hoping folks would do when we released this download.

Great. This is just one method of access, I hope people will find
compelling new uses for this data dump.

> You say performance looks fine, I would be interested what hardware you
are running it on.

I'm running it, alongside a few datasets at < 500,000 triples, on a VM with
4 gb RAM. The memory usage seems low, except when loading data (when an
extra gb or two makes all the difference). CPU hasn't been an issue yet. I
played around with clustering, but in the end, I don't need it yet. Maybe
these things become an issue when traffic is high? My biggest hurdle has
been loading the data, which I solved by breaking the file into chunks of
10,000 triples and adding RAM.

Thanks (to Richard and Karen) for the link re: attribution. I've loaded the
data triple for triple into my store, so I'm assuming the VoID meets
requirements there. I'll see about adding sensible metadata and the
recommended attribution text to the djubby displays. Even though I wish
this data were released CC-0, I'd like to meet OCLC's expectations in good
faith.

Best,

Tom

On Tue, Aug 21, 2012 at 12:42 AM, Richard Wallis <richard.wallis at oclc.org>wrote:

>  Hi Tom,
>
> It is great to see that you have done this ? just the kind of thing we
> were hoping folks would do when we released this download.
>
> You say performance looks fine, I would be interested what hardware you
> are running it on.  I have dropped 4Store on my MacBook Air and I am
> pleasantly surprised with the performance.
>
> Regarding your attribution question, you may find our attribution
> guidelines useful: http://www.oclc.org/data/attribution.html
>
> ~Richard
>
>
>
>
> On 21/08/2012 00:33, "Tom Johnson" <thomas.johnson at oregonstate.edu> wrote:
>
> A SPARQL endpoint is up at http://worldcat.library.oregonstate.edu/sparql
>
> Anyone with a SPARQL client is welcome to use that endpoint for as long as
> it exists. Performance looks just fine to me for the moment, but please let
> me know if you run into any problems.
>
> 4store provides a test query page, letting you write queries in the
> browser. It is at http://worldcat.library.oregonstate.edu/test
>
> Try something like:
>
>    DESCRIBE <http://www.worldcat.org/isbn/0879692243>
>
> or:
>
>    DESCRIBE <http://id.worldcat.org/fast/898705>
>
> The djubby front-end I put up should let you hit items by oclcnum or isbn,
> like so:
>
>    http://data.library.oregonstate.edu/worldcat/oclc/14588496
>    http://data.library.oregonstate.edu/worldcat/isbn/9780879692247
>
> It doesn't display blank nodes very intelligently, though, so I'm not sure
> how useful it will be in practice. It would be better to have something
> which will at least seek out labels for object URIs.
>
> I'm also thinking about loading in the VIAF and FAST graphs, since that
> should help with the kind of visualization Karen is talking about. In the
> meanwhile, there is an endpoint. I'll make an effort to keep it up live and
> up to date until I say otherwise, so please use it as you see fit.
>
> As an afterthought: anyone have any advice about meeting the attribution
> terms of the ODC license?
>
> - Tom
>
>
> On Fri, Aug 17, 2012 at 9:16 PM, Karen Coyle <kcoyle at kcoyle.net> wrote:
>
> Tom,
>
> I'll poke around and see if anyone is using any "easy" visualization
> software. The Nat'l Lib. of Spain did some neat stuff with Graphviz, but I
> have no idea what that took. [1]
>
> For basic functionality, I'd love to see a minimal web form that would
> launch a search via SPARQL. (Pubby may do this -- I don't see a screen shot
> that answers this for me.) It would probably have to be limited to
> searching on only certain values, but that's ok for a start. At minimum,
> pulling up everything with the subject URI of an OCLC number or an object
> with the VIAF URI. Since the URI patterns for those are set, it should be
> possible to have a form for just the number and to fill in the full URI for
> the search. An even easier alternative would be to supply the SPARQL
> patterns for those searches, to be launched from within a web page. I could
> find examples of what I mean if this isn't clear. In any case, being able
> to do some minimal searching seems to be a best first step.
>
> Thanks,
>
> kc
> [1] http://bne.linkeddata.es/graphvis/ <http://bne.linkeddata.es/graphvis/>
>
>
>
> On 8/17/12 4:14 PM, Tom Johnson wrote:
>
> I'm not having any trouble loading it (except that it is slow). I'm
> fussing with the best way to configure 4store to handle ~80 million
> triples.
>
> The data looks good to me.
>
> I'm also putting up a pubby-like front end.
>
> I'm not sure what the real cost of running a SPARQL endpoint for a
> dataset like this is going to look like, or whether I can support it in
> the long run. Still, I'm interested in hearing what people would want to
> see and how they would use it if I (or Oregon State) were to run
> services on it.
>
> - Tom
>
> On Fri, Aug 17, 2012 at 3:50 PM, Tom Morris <tfmorris at gmail.com
> <mailto:tfmorris at gmail.com> <tfmorris at gmail.com%3E>> wrote:
>
>     Karen,
>
>     On Fri, Aug 17, 2012 at 2:10 PM, Karen Coyle <kcoyle at kcoyle.net
>     <mailto:kcoyle at kcoyle.net> <kcoyle at kcoyle.net%3E>> wrote:
>      > Luc, I think this reflects an answer to your question. As with
>     much that
>      > happens in computer technology, some of us have to depend on
>     others. I find
>      > making our wishes clear helps guide those kind souls who have the
>     necessary
>      > skills. Maybe we can work further with Tom and others to spell
>     out what we
>      > need for this to be usable for us.
>
>     Does having this data loaded into a triple store help you?  What types
>     of things would it enable?
>
>     It seems like it might be marginally better than a raw RDF file, but
>     it seems like it would still take a fair amount of work to do anything
>     useful with it.
>
>     Tom
>
>     p.s. I'm curious to see if the other Tom is able to load it using his
>     tools because it looks to me like it contains invalid URIs (embedded
>     spaces) which may cause RDF tools to choke depending on how picky they
>     are about parsing.
>
>      >
>      > What we still need in the RDF world is the application that would
>     do for the
>      > Semantic Web what Mosaic did for the Web: make it viewable and
>     usable by the
>      > non-programmer. But first we have to have an actual Semantic Web,
>     and I
>      > think that's still in progress in a strict sense.
>      >
>      > kc
>      >
>      >
>      > On 8/17/12 10:31 AM, Tom Johnson wrote:
>      >>
>      >> I'm in the process of putting up a triplestore w/ endpoint
>     already. I
>      >> have no problem sending out a link.
>      >>
>      >> I'm in an all day meeting today, so it might not happen until
>     the weekend.
>      >>
>      >> On Fri, Aug 17, 2012 at 9:53 AM, Luc Gauvreau <lgovro at gmail.com
>     <mailto:lgovro at gmail.com <lgovro at gmail.com>>
>      >> <mailto:lgovro at gmail.com <lgovro at gmail.com> <
> mailto:lgovro at gmail.com>> <lgovro at gmail.com%3E%3E>> wrote:
>      >>
>      >>     Bonjour,
>      >>
>      >>     A very good question!
>      >>
>      >>     Multiple projects about linked datas and RDF, but who has the
>      >>     expertise to use it?
>      >>
>      >>     Only experts and "geeks"?
>      >>
>      >>     Is it possible for an "amateur" to use these kind of format,
>     files
>      >>     and codes?
>      >>
>      >>     A kind of "Linked data and RDF for dummies" will be very
>     usefull.
>      >>     Merci,
>      >>
>      >>     Luc Gauvreau
>      >>     (Montr?al, Qu?bec)
>      >>
>      >>
>      >>
>      >>     2012/8/17 Karen Coyle <kcoyle at kcoyle.net
>     <mailto:kcoyle at kcoyle.net <kcoyle at kcoyle.net>> <
> mailto:kcoyle at kcoyle.net <kcoyle at kcoyle.net>
>
>     <mailto:kcoyle at kcoyle.net>> <kcoyle at kcoyle.net%3E%3E>>
>      >>
>      >>
>      >>         I would love it if someone could put this in a triple
>     store for
>      >>         others to play with. How difficult is that?
>      >>
>      >>         kc
>      >>
>      >>
>      >>         On 8/17/12 8:58 AM, Jonathan Gray wrote:
>      >>
>      >>             ---------- Forwarded message ----------
>      >>             From: Richard Wallis <
> richard.wallis at dataliberate.__com
>      >>             <mailto:richard.wallis at dataliberate.com<richard.wallis at dataliberate.com><
> mailto:richard.wallis at dataliberate.com <richard.wallis at dataliberate.com>>
>     <mailto:richard.wallis at dataliberate.com<richard.wallis at dataliberate.com><
> mailto:richard.wallis at dataliberate.com <richard.wallis at dataliberate.com>>
> >>>
>      >>             Date: Fri, Aug 17, 2012 at 5:42 PM
>      >>             Subject: [LODLAM] Get Yourself a Linked Data Piece of
>      >>             WorldCat to Play With
>      >>             To: lod-lam at googlegroups.com
>     <mailto:lod-lam at googlegroups.com <lod-lam at googlegroups.com> <
> mailto:lod-lam at googlegroups.com <lod-lam at googlegroups.com>> > <
> mailto:lod-lam at googlegroups.com <lod-lam at googlegroups.com> <
> mailto:lod-lam at googlegroups.com <lod-lam at googlegroups.com>>
>     <mailto:lod-lam at googlegroups.com <lod-lam at googlegroups.com> <
> mailto:lod-lam at googlegroups.com <lod-lam at googlegroups.com>> >>
>      >>
>      >>
>      >>             In case you missed the press release earlier this week.
>      >>
>      >>             You can now download a significant number of RDF
> triples
>      >>             describing
>      >>             the most highly held 1.2 million resources in WorldCat.
>      >>               Licensed
>      >>             under ODC-BY.
>      >>
>      >>             I've posted more details on my blog:
>      >>
>      >>
>
> http://dataliberate.com/2012/__08/get-yourself-a-linked-data-__piece-of-worldcat-to-play-__with/<
> http://dataliberate.com/2012/__08/get-yourself-a-linked-data-__piece-of-worldcat-to-play-__with/>
>
>      >>
>      >>
>      >>
>     <
> http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-worldcat-to-play-with/<
> http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-worldcat-to-play-with/>
> >
>      >>
>      >>             ~Richard
>      >>
>
>
>
> _______________________________________________
> open-bibliography mailing list
> open-bibliography at lists.okfn.org
> http://lists.okfn.org/mailman/listinfo/open-bibliography
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120821/1d6b4080/attachment.htm>

From richard.wallis at oclc.org  Wed Aug 22 22:32:01 2012
From: richard.wallis at oclc.org (Richard Wallis)
Date: Wed, 22 Aug 2012 22:32:01 +0100
Subject: [open-bibliography] Fwd: [LODLAM] Get Yourself a Linked Data
 Piece of WorldCat to Play With
In-Reply-To: <CAJeHiNEsq3_Z-Z_Zo2kK3zRtKJJMh9aeS=uaoRvvuqVSSn+BKQ@mail.gmail.com>
Message-ID: <CC5B0E61.1E12%richard.wallis@oclc.org>

Hi Tom,

As the SPARQL query ?DESCRIBE <http://purl.oclc.org/dataset/WorldCat>?
shows, you have indeed loaded the triples from the VoID into your store.

Attribution text on the djubby displays would be great too.

If you have any further questions or comments, let me know.

~Richard.


On 22/08/2012 03:28, "Tom Johnson" <thomas.johnson at oregonstate.edu> wrote:

>> >?It is great to see that you have done this ? just the kind of thing we were
>> hoping folks would do when we released this download.?
> 
> Great. This is just one method of access, I hope people will find compelling
> new uses for this data dump.
> 
>> >?You say performance looks fine, I would be interested what hardware you are
>> running it on. ?
> 
> I'm running it, alongside a few datasets at < 500,000 triples, on a VM with 4
> gb RAM. The memory usage seems low, except when loading data (when an extra gb
> or two makes all the difference). CPU hasn't been an issue yet. I played
> around with clustering, but in the end, I don't need it yet. Maybe these
> things become an issue when traffic is high? My biggest hurdle has been
> loading the data, which I solved by breaking the file into chunks of 10,000
> triples and adding RAM.
> 
> Thanks (to Richard and Karen) for the link re: attribution. I've loaded the
> data triple for triple into my store, so I'm assuming the VoID meets
> requirements there. I'll see about adding sensible metadata and the
> recommended attribution text to the djubby displays. Even though I wish this
> data were released CC-0, I'd like to meet OCLC's expectations in good faith.
> 
> Best,
> 
> Tom?
> 
> On Tue, Aug 21, 2012 at 12:42 AM, Richard Wallis <richard.wallis at oclc.org>
> wrote:
>> Hi Tom,
>> 
>> It is great to see that you have done this ? just the kind of thing we were
>> hoping folks would do when we released this download.
>> 
>> You say performance looks fine, I would be interested what hardware you are
>> running it on. ?I have dropped 4Store on my MacBook Air and I am pleasantly
>> surprised with the performance.
>> 
>> Regarding your attribution question, you may find our attribution guidelines
>> useful: http://www.oclc.org/data/attribution.html
>> 
>> ~Richard
>> 
>> 
>> 
>> 
>> On 21/08/2012 00:33, "Tom Johnson" <thomas.johnson at oregonstate.edu
>> <http://thomas.johnson at oregonstate.edu> > wrote:
>> 
>>> A SPARQL endpoint is up at http://worldcat.library.oregonstate.edu/sparql?
>>> 
>>> Anyone with a SPARQL client is welcome to use that endpoint for as long as
>>> it exists. Performance looks just fine to me for the moment, but please let
>>> me know if you run into any problems.
>>> 
>>> 4store provides a test query page, letting you write queries in the browser.
>>> It is at http://worldcat.library.oregonstate.edu/test
>>> 
>>> Try something like:
>>> 
>>> ? ?DESCRIBE <http://www.worldcat.org/isbn/0879692243>
>>> 
>>> or:?
>>> 
>>> ? ?DESCRIBE <http://id.worldcat.org/fast/898705>
>>> 
>>> The djubby front-end I put up should let you hit items by oclcnum or isbn,
>>> like so:
>>> 
>>> ? ?http://data.library.oregonstate.edu/worldcat/oclc/14588496
>>> ? ?http://data.library.oregonstate.edu/worldcat/isbn/9780879692247
>>> 
>>> It doesn't display blank nodes very intelligently, though, so I'm not sure
>>> how useful it will be in practice. It would be better to have something
>>> which will at least seek out labels for object URIs.?
>>> 
>>> I'm also thinking about loading in the VIAF and FAST graphs, since that
>>> should help with the kind of visualization Karen is talking about. In the
>>> meanwhile, there is an endpoint. I'll make an effort to keep it up live and
>>> up to date until I say otherwise, so please use it as you see fit.
>>> 
>>> As an afterthought: anyone have any advice about meeting the attribution
>>> terms of the ODC license?
>>> 
>>> - Tom
>>> 
>>> 
>>> On Fri, Aug 17, 2012 at 9:16 PM, Karen Coyle <kcoyle at kcoyle.net
>>> <http://kcoyle at kcoyle.net> > wrote:
>>>> Tom,
>>>> 
>>>> I'll poke around and see if anyone is using any "easy" visualization
>>>> software. The Nat'l Lib. of Spain did some neat stuff with Graphviz, but I
>>>> have no idea what that took. [1]
>>>> 
>>>> For basic functionality, I'd love to see a minimal web form that would
>>>> launch a search via SPARQL. (Pubby may do this -- I don't see a screen shot
>>>> that answers this for me.) It would probably have to be limited to
>>>> searching on only certain values, but that's ok for a start. At minimum,
>>>> pulling up everything with the subject URI of an OCLC number or an object
>>>> with the VIAF URI. Since the URI patterns for those are set, it should be
>>>> possible to have a form for just the number and to fill in the full URI for
>>>> the search. An even easier alternative would be to supply the SPARQL
>>>> patterns for those searches, to be launched from within a web page. I could
>>>> find examples of what I mean if this isn't clear. In any case, being able
>>>> to do some minimal searching seems to be a best first step.
>>>> 
>>>> Thanks,
>>>> 
>>>> kc
>>>> [1] http://bne.linkeddata.es/graphvis/ <http://bne.linkeddata.es/graphvis/>
>>>> 
>>>> 
>>>> On 8/17/12 4:14 PM, Tom Johnson wrote:
>>>>> I'm not having any trouble loading it (except that it is slow). I'm
>>>>> fussing with the best way to configure 4store to handle ~80 million
>>>>> triples.
>>>>> 
>>>>> The data looks good to me.
>>>>> 
>>>>> I'm also putting up a pubby-like front end.
>>>>> 
>>>>> I'm not sure what the real cost of running a SPARQL endpoint for a
>>>>> dataset like this is going to look like, or whether I can support it in
>>>>> the long run. Still, I'm interested in hearing what people would want to
>>>>> see and how they would use it if I (or Oregon State) were to run
>>>>> services on it.
>>>>> 
>>>>> - Tom
>>>>> 
>>>>> On Fri, Aug 17, 2012 at 3:50 PM, Tom Morris <tfmorris at gmail.com
>>>>> <http://tfmorris at gmail.com>
>>>>> <mailto:tfmorris at gmail.com> <mailto:tfmorris at gmail.com%3E> > wrote:
>>>>> 
>>>>> ? ? Karen,
>>>>> 
>>>>> ? ? On Fri, Aug 17, 2012 at 2:10 PM, Karen Coyle <kcoyle at kcoyle.net
>>>>> <http://kcoyle at kcoyle.net>
>>>>> ? ? <mailto:kcoyle at kcoyle.net> <mailto:kcoyle at kcoyle.net%3E> > wrote:
>>>>> ? ? ?> Luc, I think this reflects an answer to your question. As with
>>>>> ? ? much that
>>>>> ? ? ?> happens in computer technology, some of us have to depend on
>>>>> ? ? others. I find
>>>>> ? ? ?> making our wishes clear helps guide those kind souls who have the
>>>>> ? ? necessary
>>>>> ? ? ?> skills. Maybe we can work further with Tom and others to spell
>>>>> ? ? out what we
>>>>> ? ? ?> need for this to be usable for us.
>>>>> 
>>>>> ? ? Does having this data loaded into a triple store help you? ?What types
>>>>> ? ? of things would it enable?
>>>>> 
>>>>> ? ? It seems like it might be marginally better than a raw RDF file, but
>>>>> ? ? it seems like it would still take a fair amount of work to do anything
>>>>> ? ? useful with it.
>>>>> 
>>>>> ? ? Tom
>>>>> 
>>>>> ? ? p.s. I'm curious to see if the other Tom is able to load it using his
>>>>> ? ? tools because it looks to me like it contains invalid URIs (embedded
>>>>> ? ? spaces) which may cause RDF tools to choke depending on how picky they
>>>>> ? ? are about parsing.
>>>>> 
>>>>> ? ? ?>
>>>>> ? ? ?> What we still need in the RDF world is the application that would
>>>>> ? ? do for the
>>>>> ? ? ?> Semantic Web what Mosaic did for the Web: make it viewable and
>>>>> ? ? usable by the
>>>>> ? ? ?> non-programmer. But first we have to have an actual Semantic Web,
>>>>> ? ? and I
>>>>> ? ? ?> think that's still in progress in a strict sense.
>>>>> ? ? ?>
>>>>> ? ? ?> kc
>>>>> ? ? ?>
>>>>> ? ? ?>
>>>>> ? ? ?> On 8/17/12 10:31 AM, Tom Johnson wrote:
>>>>> ? ? ?>>
>>>>> ? ? ?>> I'm in the process of putting up a triplestore w/ endpoint
>>>>> ? ? already. I
>>>>> ? ? ?>> have no problem sending out a link.
>>>>> ? ? ?>>
>>>>> ? ? ?>> I'm in an all day meeting today, so it might not happen until
>>>>> ? ? the weekend.
>>>>> ? ? ?>>
>>>>> ? ? ?>> On Fri, Aug 17, 2012 at 9:53 AM, Luc Gauvreau <lgovro at gmail.com
>>>>> <http://lgovro at gmail.com>
>>>>> ? ? <mailto:lgovro at gmail.com>
>>>>> ? ? ?>> <mailto:lgovro at gmail.com <mailto:lgovro at gmail.com>>
>>>>> <mailto:lgovro at gmail.com%3E%3E> > wrote:
>>>>> ? ? ?>>
>>>>> ? ? ?>> ? ? Bonjour,
>>>>> ? ? ?>>
>>>>> ? ? ?>> ? ? A very good question!
>>>>> ? ? ?>>
>>>>> ? ? ?>> ? ? Multiple projects about linked datas and RDF, but who has the
>>>>> ? ? ?>> ? ? expertise to use it?
>>>>> ? ? ?>>
>>>>> ? ? ?>> ? ? Only experts and "geeks"?
>>>>> ? ? ?>>
>>>>> ? ? ?>> ? ? Is it possible for an "amateur" to use these kind of format,
>>>>> ? ? files
>>>>> ? ? ?>> ? ? and codes?
>>>>> ? ? ?>>
>>>>> ? ? ?>> ? ? A kind of "Linked data and RDF for dummies" will be very
>>>>> ? ? usefull.
>>>>> ? ? ?>> ? ? Merci,
>>>>> ? ? ?>>
>>>>> ? ? ?>> ? ? Luc Gauvreau
>>>>> ? ? ?>> ? ? (Montr?al, Qu?bec)
>>>>> ? ? ?>>
>>>>> ? ? ?>>
>>>>> ? ? ?>>
>>>>> ? ? ?>> ? ? 2012/8/17 Karen Coyle <kcoyle at kcoyle.net
>>>>> <http://kcoyle at kcoyle.net>
>>>>> ? ? <mailto:kcoyle at kcoyle.net> <mailto:kcoyle at kcoyle.net
>>>>> 
>>>>> ? ? <mailto:kcoyle at kcoyle.net>> <mailto:kcoyle at kcoyle.net%3E%3E> >
>>>>> ? ? ?>>
>>>>> ? ? ?>>
>>>>> ? ? ?>> ? ? ? ? I would love it if someone could put this in a triple
>>>>> ? ? store for
>>>>> ? ? ?>> ? ? ? ? others to play with. How difficult is that?
>>>>> ? ? ?>>
>>>>> ? ? ?>> ? ? ? ? kc
>>>>> ? ? ?>>
>>>>> ? ? ?>>
>>>>> ? ? ?>> ? ? ? ? On 8/17/12 8:58 AM, Jonathan Gray wrote:
>>>>> ? ? ?>>
>>>>> ? ? ?>> ? ? ? ? ? ? ---------- Forwarded message ----------
>>>>> ? ? ?>> ? ? ? ? ? ? From: Richard Wallis
>>>>> <richard.wallis at dataliberate.__com
>>>>> <http://richard.wallis at dataliberate.__com>
>>>>> ? ? ?>> ? ? ? ? ? ? <mailto:richard.wallis at dataliberate.com
>>>>> <mailto:richard.wallis at dataliberate.com>
>>>>> ? ? <mailto:richard.wallis at dataliberate.com
>>>>> <mailto:richard.wallis at dataliberate.com> >>>
>>>>> ? ? ?>> ? ? ? ? ? ? Date: Fri, Aug 17, 2012 at 5:42 PM
>>>>> ? ? ?>> ? ? ? ? ? ? Subject: [LODLAM] Get Yourself a Linked Data Piece of
>>>>> ? ? ?>> ? ? ? ? ? ? WorldCat to Play With
>>>>> ? ? ?>> ? ? ? ? ? ? To: lod-lam at googlegroups.com
>>>>> <http://lod-lam at googlegroups.com>
>>>>> ? ? <mailto:lod-lam at googlegroups.com <mailto:lod-lam at googlegroups.com> >
>>>>> <mailto:lod-lam at googlegroups.com <mailto:lod-lam at googlegroups.com>
>>>>> ? ? <mailto:lod-lam at googlegroups.com <mailto:lod-lam at googlegroups.com> >>
>>>>> ? ? ?>>
>>>>> ? ? ?>>
>>>>> ? ? ?>> ? ? ? ? ? ? In case you missed the press release earlier this
>>>>> week.
>>>>> ? ? ?>>
>>>>> ? ? ?>> ? ? ? ? ? ? You can now download a significant number of RDF
>>>>> triples
>>>>> ? ? ?>> ? ? ? ? ? ? describing
>>>>> ? ? ?>> ? ? ? ? ? ? the most highly held 1.2 million resources in
>>>>> WorldCat.
>>>>> ? ? ?>> ? ? ? ? ? ? ? Licensed
>>>>> ? ? ?>> ? ? ? ? ? ? under ODC-BY.
>>>>> ? ? ?>>
>>>>> ? ? ?>> ? ? ? ? ? ? I've posted more details on my blog:
>>>>> ? ? ?>>
>>>>> ? ? ?>>
>>>>> ? ? 
>>>>> http://dataliberate.com/2012/__08/get-yourself-a-linked-data-__piece-of-wo
>>>>> rldcat-to-play-__with/
>>>>> <http://dataliberate.com/2012/__08/get-yourself-a-linked-data-__piece-of-w
>>>>> orldcat-to-play-__with/>
>>>>> ? ? ?>>
>>>>> ? ? ?>>
>>>>> ? ? ?>>
>>>>> ? ? 
>>>>> <http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-world
>>>>> cat-to-play-with/
>>>>> <http://dataliberate.com/2012/08/get-yourself-a-linked-data-piece-of-world
>>>>> cat-to-play-with/> >
>>>>> ? ? ?>>
>>>>> ? ? ?>> ? ? ? ? ? ? ~Richard
>>>>> ? ? ?>>
>>>>> 
>>>>> 
>> 
>> _______________________________________________
>> open-bibliography mailing list
>> open-bibliography at lists.okfn.org
>> http://lists.okfn.org/mailman/listinfo/open-bibliography
>> 
> 
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120822/4681b326/attachment-0001.htm>

From kcoyle at kcoyle.net  Wed Aug 22 22:54:38 2012
From: kcoyle at kcoyle.net (Karen Coyle)
Date: Wed, 22 Aug 2012 14:54:38 -0700
Subject: [open-bibliography] Fwd: NISO Publishes Journal Article Tag Suite
	(JATS) Standard
In-Reply-To: <016101cd80aa$022865d0$06793170$@org>
References: <016101cd80aa$022865d0$06793170$@org>
Message-ID: <5035551E.4090408@kcoyle.net>

This may not change anything in the bibsoup work, but being a NISO 
standard will reinforce the NLM DTD as journal data.

kc


-------- Original Message --------
Subject: 	NISO Publishes Journal Article Tag Suite (JATS) Standard
Date: 	Wed, 22 Aug 2012 17:06:32 -0400
From: 	Cynthia Hodgson <chodgson at niso.org>
Reply-To: 	<chodgson at niso.org>
Organization: 	NISO
To: 	<newsline at list.niso.org>



*NISO Publishes Journal Article Tag Suite (JATS) Standard**/
Provides common XML format for exchanging journal content/*

**

The National Information Standards Organization (NISO) announces the
publication of a new American National Standard, /JATS: Journal Article
Tag Suite/, ANSI/NISO Z39.96-2012. JATS provides a common XML format in
which publishers and archives can exchange journal content by preserving
the intellectual content of journals independent of the form in which
that content was originally delivered. In addition to the element and
attribute descriptions, three journal article tag sets (the /Archiving
and Interchange Tag Set/, the /Journal Publishing Tag Set/, and the
/Article Authoring Tag Set/) are part of the standard. While designed to
describe the textual and graphical content of journal articles, it can
also be used for some other materials, such as letters, editorials, and
book and product reviews.

"Although this is the first version of JATS as an American National
Standard," stated Nettie Lagace, NISO Associate Director for Programs,
"the specification has a long history as the /National Library of
Medicine (NLM) Journal Archiving and Interchange Tag Suite/, commonly
referred to as the NLM DTDs. Those DTDs were based on an article model
that was used in the National Center for Biotechnology Information
(NCBI)/NLM PubMed Central project to archive life science journals. The
original PubMed Central article model was expanded in scope with support
from Harvard University Libraries and The Andrew W. Mellon Foundation,
in collaboration with Inera, Inc. and Mulberry Technologies, Inc.,
resulting in 2003 in the full /NLM Journal Archiving and Interchange Tag
Suite/. The Tag Suite had reached version 3.0 prior to initiation of the
NISO standardization process."

"Since its initial release, the /Archiving and Interchange Tag Suite/
has been widely popular," said B. Tommie Usdin, President of Mulberry
Technologies, Inc. and Co-chair of the NISO JATS Working Group. "The
format is being used to tag thousands of journals worldwide and is used
for the journal archives at PubMed Central and Portico and by the online
publisher HighWire Press. The Library of Congress and the British
Library have announced their intention to use these models for archiving
electronic content."

"Taking JATS through the NISO standardization process will bring
awareness of the Tag Suite to a larger and more varied audience,"
explained Jeffrey Beck, NCBI Technical Information Specialist at the
National Library of Medicine and Co-chair of the NISO JATS Working
Group. "We expect this wider audience will find uses for the Tag Suite
in new applications, beyond its traditional uses in journal publishing
and archiving."

"We are pleased that the NLM project team brought this valuable standard
to NISO for wider dissemination," stated Todd Carpenter, NISO Executive
Director. "We will be supporting a standing committee to continuously
update the standard and NLM will continue to host the user documentation
and schemas that support the standard."

The JATS standard is available as both an online XML document and a
freely downloadable PDF from the NISO website
(www.niso.org/workrooms/journalmarkup
<http://www.niso.org/workrooms/journalmarkup>). Supporting documentation
and schemas in DTD, RELAX NG, and W3C Schema formats are available at:
jats.nlm.nih.gov/ <http://jats.nlm.nih.gov/>.





From naomi.lillie at okfn.org  Thu Aug 23 14:00:34 2012
From: naomi.lillie at okfn.org (Naomi Lillie)
Date: Thu, 23 Aug 2012 14:00:34 +0100
Subject: [open-bibliography] Project write-up
Message-ID: <CACGRf4J0c0j7YSSVGPskXsVLyQWx8p60_Yaj=4x1X8qcKWuipg@mail.gmail.com>

Dear all,

The write-up of the JISC Open Biblio 2 project is now live:
http://openbiblio.net/2012/08/23/final-report-jisc-open-bibliography-2/

Thanks to you all for your input in making this such a success!

Regards,
Naomi


-- 
Naomi Lillie
Foundation Administrator and Community Coordinator
Open Knowledge Foundation
http://okfn.org/
Skype: n.lillie
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120823/2b84ba21/attachment.htm>

From tfmorris at gmail.com  Thu Aug 23 18:05:57 2012
From: tfmorris at gmail.com (Tom Morris)
Date: Thu, 23 Aug 2012 13:05:57 -0400
Subject: [open-bibliography] [openbiblio-dev] Project write-up
In-Reply-To: <CACGRf4J0c0j7YSSVGPskXsVLyQWx8p60_Yaj=4x1X8qcKWuipg@mail.gmail.com>
References: <CACGRf4J0c0j7YSSVGPskXsVLyQWx8p60_Yaj=4x1X8qcKWuipg@mail.gmail.com>
Message-ID: <CAE9vqEFP_RODNhbyknzGEp0M6g0vRhECOECHOQwwyMH=Va3DWQ@mail.gmail.com>

On Thu, Aug 23, 2012 at 9:00 AM, Naomi Lillie <naomi.lillie at okfn.org> wrote:
> Dear all,
>
> The write-up of the JISC Open Biblio 2 project is now live:
> http://openbiblio.net/2012/08/23/final-report-jisc-open-bibliography-2/
>
> Thanks to you all for your input in making this such a success!

Thanks for the summary.  What happens next?  Is there any follow-on
work planned?  The "future work" section seems to talk about work in a
different space ("open science").

I also had two questions based on a quick skim of the linked blog posts:

1. Pubcrawler has harvested 10M bibliographic records - where are they
located? Are they updated regularly?
http://openbiblio.net/2012/06/13/pubcrawler-finding-research-publications/

2. BNB has been loaded into BibServer - what's the URL for this?
http://openbiblio.net/2012/06/18/bringing-the-open-german-national-bibliography-to-a-bibserver/

Tom


From pm286 at cam.ac.uk  Thu Aug 23 18:10:18 2012
From: pm286 at cam.ac.uk (Peter Murray-Rust)
Date: Thu, 23 Aug 2012 18:10:18 +0100
Subject: [open-bibliography] [openbiblio-dev] Project write-up
In-Reply-To: <CAE9vqEFP_RODNhbyknzGEp0M6g0vRhECOECHOQwwyMH=Va3DWQ@mail.gmail.com>
References: <CACGRf4J0c0j7YSSVGPskXsVLyQWx8p60_Yaj=4x1X8qcKWuipg@mail.gmail.com>
	<CAE9vqEFP_RODNhbyknzGEp0M6g0vRhECOECHOQwwyMH=Va3DWQ@mail.gmail.com>
Message-ID: <CAD2k14NBQ=bSRV5YJ2yV9h-A=uKCzL7Go+DKD8p585EWhOx0MQ@mail.gmail.com>

On Thu, Aug 23, 2012 at 6:05 PM, Tom Morris <tfmorris at gmail.com> wrote:

> On Thu, Aug 23, 2012 at 9:00 AM, Naomi Lillie <naomi.lillie at okfn.org>
> wrote:
> > Dear all,
> >
> > The write-up of the JISC Open Biblio 2 project is now live:
> > http://openbiblio.net/2012/08/23/final-report-jisc-open-bibliography-2/
> >
> > Thanks to you all for your input in making this such a success!
>
> Thanks for the summary.  What happens next?  Is there any follow-on
> work planned?  The "future work" section seems to talk about work in a
> different space ("open science").
>

I personally am looking for any source which might use it. For me - who
can't now apply for research grants I am looking to promote its use in
managing metrics for  research funding and trying to interest funders. I am
also looking to seee if the VIVO community might take it up.

>
> I also had two questions based on a quick skim of the linked blog posts:
>
> 1. Pubcrawler has harvested 10M bibliographic records - where are they
> located? Are they updated regularly?
> http://openbiblio.net/2012/06/13/pubcrawler-finding-research-publications/
>

This is very important. I don't think it's regular but I want to explore
BiomedCentral as a new publisher. I talked with them yesterday and I plan
to visit shortly and present to them.

>
>

-- 
Peter Murray-Rust
Reader in Molecular Informatics
Unilever Centre, Dep. Of Chemistry
University of Cambridge
CB2 1EW, UK
+44-1223-763069
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120823/6e62799e/attachment.htm>

From naomi.lillie at okfn.org  Tue Aug 28 10:49:35 2012
From: naomi.lillie at okfn.org (Naomi Lillie)
Date: Tue, 28 Aug 2012 10:49:35 +0100
Subject: [open-bibliography] Fwd: [open-heritage] Announcing the Open
 Culture and Open Science Hack at OKFestival
In-Reply-To: <CAOn=SAZ82ogb=f5Wjh6fVu8_KKDPcyzqXwOyptN9prSF5T851w@mail.gmail.com>
References: <CAOn=SAZ82ogb=f5Wjh6fVu8_KKDPcyzqXwOyptN9prSF5T851w@mail.gmail.com>
Message-ID: <CACGRf4+z7d0JKZDdrdPffEw9K=jLjHKPAjkFVJRipE0Dod3ZzQ@mail.gmail.com>

Hi all,

For those of you making the trip out to Helsinki (3 weeks to go!) I thought
you might be interested in a Hackday run by the Open GLAM stream - see
below for details.

Regards,
Naomi


---------- Forwarded message ----------
From: Sam Leon <sam.leon at okfn.org>
Date: 28 August 2012 10:46
Subject: [open-heritage] Announcing the Open Culture and Open Science Hack
at OKFestival
To: "open-glam at lists.okfn.org" <open-GLAM at lists.okfn.org>


For those who don't already know the Open Knowledge Foundation will be
running the world's first Open Knowledge Festival
<http://okfestival.org/>in under three weeks time in Helsinki,
Finland.

As part of the Cultural Heritage stream we will be running a hackday
on *Tuesday
18th September* in collaboration with the Open Knowledge Foundation's open
science team. Full details of this are now up on the OKFestival
website<http://okfestival.org/open-culture-and-science-hackday/>
.

There will be a host of activities including hacking on Finnish cultural
heritage datasets opened up specially for the festival, hacking on existing
tools (such as TEXTUS <http://textusproject.org>,
PyBossa<http://pybossa.com/>and
Pundit <http://thepund.it/>) as well as plenty of activities for non-coders
such as a Wikipedia edit-a-thon and an open video mash-up session.

For all those making the trip out to Helsinki, please *sign up
here*<https://docs.google.com/spreadsheet/viewform?formkey=dDZfTlFNaEpzOUlRbjQ3NjRoNkwyQ0E6MQ#gid=0>.
For those not able to make it there will be way of participating remotely,
more details to follow shortly.

All the best,
Sam

-- 
Sam Leon
Community Coordinator
Open Knowledge Foundation
http://okfn.org/
Twitter: @noeL_maS
Skype: samedleon


_______________________________________________
open-glam mailing list
open-glam at lists.okfn.org
http://lists.okfn.org/mailman/listinfo/open-glam



-- 
Naomi Lillie
Foundation Administrator and Community Coordinator
Open Knowledge Foundation
http://okfn.org/
Skype: n.lillie
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.okfn.org/pipermail/open-bibliography/attachments/20120828/265c7def/attachment.htm>

